{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec09daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e780f5",
   "metadata": {},
   "source": [
    "# Symmetries in RL - Practical\n",
    "\n",
    "\n",
    "Aims of the practical\n",
    "- understand symmetries in MPD\n",
    "- design policies that encode these symmetries (with less parameters)\n",
    "- try them out (if time allows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f5084",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Policy gradients (PG) allow for a great flexibility in the parameterization of policies to solve MDPs.\n",
    "\n",
    "Policy gradients algorithms rely on the PG theorem which expresses the gradient of the RL loss (here expected sum of discounted future loss $V_{\\theta} = \\mathbb{E}[\\sum_i^{\\infty} \\gamma^i r_i]$) in a form amenable to sample estimation.\n",
    "Noting $\\theta\\in \\Theta$ the paramaters of a class of policies $\\{\\pi_{\\theta} \\mid \\theta \\in \\Theta \\}$.\n",
    "\n",
    "$$\\nabla_\\theta \\, V_{\\theta} = \\mathbb{E}_{\\pi} [Q^\\pi(s,a) \\nabla_{\\theta} \\log\\,\\pi(a|s)]$$\n",
    "\n",
    "In PG algorithms, iterative updates to $\\theta$ are made by following (an estimate of) the negative gradient $g_{\\theta} = -\\nabla_\\theta \\, V_{\\theta}$ until convergence.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha g_{\\theta}$$\n",
    "\n",
    "\n",
    "In particular, it can be used to encode prior information about the system into the policy.\n",
    "\n",
    "The aim of this notebook is to explore ways to introduce *symmetry* assumptions about the MDP into the policy.\n",
    "This notebook is based on the paper: [MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning](https://arxiv.org/abs/2006.16908) by van der Pol et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bb715",
   "metadata": {},
   "source": [
    "## Symmetries in RL: a brief intro\n",
    "\n",
    "Cartpole example\n",
    "* state $s = [x, \\dot{x}, \\theta, \\dot{\\theta}]$ (cart position, pole angle, derivatives)\n",
    "* action $a \\in \\{\\leftarrow, \\rightarrow\\}$, (lateral force on the cart)\n",
    "\n",
    "There is a symmetry around $s_c = [0,0,0,0]$.\n",
    "Consider \n",
    "* the reflexion operator $L[s] = -s$\n",
    "* the swap operator on a binary policy $\\pi=[p, 1-p]$:  $K[\\pi] = [1-p, p]$\n",
    "\n",
    "The optimal policy $\\pi^*$ can be shown to satisfy\n",
    "$$ K[\\pi^*(s)] = \\pi^*(L[s])$$\n",
    "\n",
    "In other words\n",
    "$$\\pi^*(\\leftarrow|s) = \\pi^*(\\rightarrow|-s) $$\n",
    "\n",
    "<img src=\"pictures/mdp_hom.png\" width=300  />\n",
    "\n",
    "\n",
    "The consequence is you interactions on both sides of the point of symmetry can inform the same simpler policy $\\bar{\\pi}$ defined for  $s \\in (\\mathbb{R}^{+})^{4}$, and lead to better **sample efficiency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0db6c",
   "metadata": {},
   "source": [
    "## Understanding Invariance and Equivariance\n",
    "\n",
    "### Prelimimary : Group\n",
    "\n",
    "you may want a refresher: [wikipedia](https://en.wikipedia.org/wiki/Group_(mathematics)#Definition)\n",
    "\n",
    "---\n",
    "\n",
    "Let $G$ be a group indexing a set of transformations operators $L_g : X \\to X$, $g \\in G$ \n",
    "\n",
    "Let $f$ be a mapping from $X$ to $Y$\n",
    "\n",
    "### Invariance\n",
    "\n",
    "$f$ is invariant or symmetric to $L_g$ if $f(x) = f(L_g[x])$, for all $g \\in G$, $x \\in X$\n",
    "\n",
    "$\\{L_g\\}_{g\\in G}$ is a set of symmetries of $f$ \n",
    "\n",
    "For example, convolutional networks are invariant to translation of the input.\n",
    "\n",
    "\n",
    "\n",
    "### Equivariance\n",
    "\n",
    "$f$ is equivariant to $L_g$ if there exists a __second__ transformation operator $K_g : Y \\to Y$ in the output space of $f$ such that \n",
    "\n",
    "$\\quad K_g[f(x)] = f (L_g [x])$, for all $g \\in G, x \\in X$ .\n",
    "\n",
    "\n",
    "This is a good property to have in image segmentation models with respect to translations and rotations (with $K_g=L_g$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68252c53",
   "metadata": {},
   "source": [
    "## Identifying the Symmetries of an MDP\n",
    "\n",
    "**MDP with symmetries**. \n",
    "\n",
    "In an MDP with symmetries there is a set of transformations on the state-action space, which leaves the reward function and transition operator invariant. We define a state transformation and a state-dependent action transformation as $L_g : S \\to S$ and $K_g^s : A \\to A$ respectively. Invariance of the reward\n",
    "function and transition function is then characterized as\n",
    "\n",
    "$\\quad\\quad R(s, a) = R(L_g [s], K^s_g [a])$ for all $g \\in G, s \\in S, \\in  A$ \n",
    "\n",
    "$\\quad\\quad T (s′|s, a) = T (L_g [s′]|L_g [s], K^s_g [a])$ for all $g \\in G, s \\in S, a \\in A.$ \n",
    "\n",
    "\n",
    "For the cartpole example, there are 2 pairs of input / output transformations to that leave the optimal policy unchanged\n",
    "\n",
    "<img src=\"pictures/mdp_hom_half.png\" width=500  />\n",
    "\n",
    "\n",
    "**Question**: can you figure and formalize these?\n",
    "\n",
    "- the identity / identity pair.\n",
    "- the reflexion around $s_c= (0,0,0,0)$ / swap of policy outcomes\n",
    "\n",
    "Let's code these up. Both of these can be written, respectively, as matrix operations on the state and policy output vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cartpole_state_group_representations() -> List[torch.TensorType]:\n",
    "    \"\"\"\n",
    "    Matrix representation of the group symmetry on the state: \n",
    "    * identity\n",
    "    * a multiplication of all state variables by -1\n",
    "    \n",
    "    return: a list of two 4*4 matrices\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def get_cartpole_action_group_representations() -> List[torch.TensorType]:\n",
    "    \"\"\"\n",
    "    Representation of the group symmetry on the policy: \n",
    "    * identity\n",
    "    * a permutation of the actions\n",
    "    \n",
    "    return: a list of two 2*2 matrices\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10165c17",
   "metadata": {},
   "source": [
    "Let's test state and action group representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caab2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell just checks the function you coded work as intended.\n",
    "# More precisely it checks that transformations form a group and can be composed.\n",
    "\n",
    "state_group_reps = get_cartpole_state_group_representations()\n",
    "action_group_reps = get_cartpole_action_group_representations()\n",
    "\n",
    "# picks two indices randomly\n",
    "i,j = np.random.randint(0, len(state_group_reps)-1, size=2)\n",
    "\n",
    "# check that composition of 2 elements in the group, stays in the group\n",
    "for group_rep in [state_group_reps, action_group_reps]:\n",
    "    element1 = group_rep[i]\n",
    "    element2 = group_rep[j]\n",
    "    \n",
    "    composed_element = your code here (use torch.matmul)\n",
    "    \n",
    "    assert any([torch.equal(composed_element, g) for g in group_rep])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b402e60",
   "metadata": {},
   "source": [
    "## Building Equivariant Layers for a policy\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "Having identified the pairs of transformations, we know the optimal policy will satisfy the equivariance property: \n",
    "\n",
    "$$ K_g[\\pi^*(s)] = \\pi^*(L_g[s]), \\forall g \\in G$$\n",
    "\n",
    "we can build a neural network layer satisfying this property with fewer parameters and better generalization properties than a network that does not make this assumption\n",
    "\n",
    "### First step: Linear Network\n",
    "\n",
    "Classic (linear) Neural network layer: $ z' = W z + b$. \n",
    "\n",
    "To simplify the math, we merge the bias into the weights so $W \\to [W, b]$ and $z \\to [z, 1]$. This leads to:\n",
    "\n",
    "$$z' = W z$$\n",
    "\n",
    "For a given pair of linear group transformation operators in matrix form $(L_g , K_g)$, where $L_g$ is the input transformation and $K_g$ is the output transformation, we then have to solve the equation\n",
    "\n",
    "$$K_g W z = W L_g z, \\forall g \\in G, z$$\n",
    "\n",
    "\n",
    "Space of Equivariant weights\n",
    "$$W_{eq} = \\{ W ∈ W_{total} | K_g W = W L_g , \\forall g \\in G\\}$$\n",
    "\n",
    "\n",
    "Symmetrizer of weights\n",
    "$$S(W) = \\frac{1}{|G|}\\sum_{g\\in G} K_g^{−1}  W L_g $$\n",
    "\n",
    "We have that\n",
    "$$\\forall W \\in W_{total}, S(W) \\in W_{eq}$$\n",
    "\n",
    "\n",
    "**What about the non-linearity?**: \n",
    "Point-wise nonlinearity do not change the equivariance property for permuations!\n",
    "\n",
    "**Questions:**\n",
    "* if we use $[\\phi_1(s), \\phi_2(s)] = W s + b$ (a linear network), what is the size of $W$? \n",
    "* What is the dimension of $W_{total}$?\n",
    "* What about the dimension of $W_{eq}$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a pre-existing neural network layer into an equivariant layer \n",
    "# via 'symmetrization' given the identified invariances\n",
    "\n",
    "def symmetrize(W: torch.TensorType, group: List[torch.TensorType]) -> torch.TensorType:\n",
    "    \"\"\"\n",
    "    Create equivariant weight matrix\n",
    "    INPUT\n",
    "    :param W: input weight of size 2 x 4\n",
    "    :group: the invariance representation\n",
    "    OUTPUT\n",
    "    the symmetrized weight of size 2 x 4\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError\n",
    "        \n",
    "        \n",
    "# Let's check shapes\n",
    "group = [get_cartpole_state_group_representations(),\n",
    "         get_cartpole_action_group_representations()]\n",
    "W = torch.tensor(np.random.rand(2, 4).astype(np.float32))\n",
    "W_sym = symmetrize(W, group)\n",
    "\n",
    "assert W.shape == W_sym.shape\n",
    "\n",
    "print('original weights:')\n",
    "print(W)\n",
    "print('symmetrized weights:')\n",
    "print(W_sym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7ff00",
   "metadata": {},
   "source": [
    "Let's check the symmetrized weights do indeed parameterize a policy with the desired equivariance property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7983792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the following helper function\n",
    "def test_network_is_equivariant(W: torch.TensorType, z: torch.TensorType, group: List[torch.TensorType]) -> None:\n",
    "    \"\"\" testing for a specific input z \n",
    "    INPUT\n",
    "    :param W: input weight of size 2 x 4\n",
    "    :param z: input state vector of size 4\n",
    "    :param group: the invariance representation\n",
    "    \"\"\"\n",
    "    \n",
    "    is_equivariant = []\n",
    "    for i in range(len(group[0])):\n",
    "\n",
    "        # here we had bias and non-linearity\n",
    "        b = 1 # bias\n",
    "        phi = lambda x: torch.relu(x) # pointwise non linearity\n",
    "        \n",
    "        L_g = group[0][i] # input transformation\n",
    "        Lz = torch.matmul(L_g, z)\n",
    "        WLz = phi(torch.matmul(W, Lz) + b)\n",
    "        Wz = phi(torch.matmul(W, z) + b)\n",
    "        K_g = group[1][i] # output transformation\n",
    "        KWz = torch.matmul(K_g, Wz)\n",
    "\n",
    "        is_equivariant.append(torch.equal(KWz, WLz))\n",
    "    assert all(is_equivariant)\n",
    "    print('network is equivariant!')\n",
    "\n",
    "    \n",
    "# sample random state\n",
    "z = torch.tensor(np.random.rand(4,1).astype(np.float32))\n",
    "\n",
    "# use the test function on this state for the symmetrized weights\n",
    "test_network_is_equivariant(W_sym, z, group) # test should pass!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e2879",
   "metadata": {},
   "source": [
    "## Building a basis for Equivariant layers\n",
    "\n",
    "When working with discrete state actions, the set of equivariant weight\n",
    "$$W_{eq} = \\{ W ∈ W_{total} | K_g W = W L_g , \\forall g \\in G\\}$$\n",
    "\n",
    "is a linear subspace of the space of weights $W_{total}$.\n",
    "\n",
    "To parameterize an equivariant layer, it is enough to express the weights in a basis of $W_{eq}$\n",
    "\n",
    "\n",
    "**How to find a basis for $W_{eq}$?**\n",
    "One procedure consists in sampling enough weights from $W_{eq}$ such that their span (almost surely) equals $w_{eq}$, in other words, they cover all directions of $W_{eq}$. If such a sampling procedure is available, then the problem boils down to finding a basis for a collection of vectors, which is a simpler problem.\n",
    "\n",
    "\n",
    "A basis for $W_{eq}$ can be constructed using the following procedure\n",
    "* 1) sample non-equivariant weights  $(W_n)_{n=1..N}$\n",
    "* 2) symmetrize those weights $\\tilde{W}_n = S(W_n)$\n",
    "* 3) find a basis for $W_{eq}$ from $(\\tilde{W}_n)_{n=1..N}$, i.e. find $\\{V_i\\}_{i=1}^r$ such that $\\forall w \\in W_{eq}, \\exists c \\in \\mathbb{R}^r, w = \\sum_i c_i V_i$ (and use $c$ as a parameter vector)\n",
    "\n",
    "\n",
    "Let's do this for the cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equivariant_basis(state_group: List[torch.TensorType], action_group: List[torch.TensorType], sample_size: int):\n",
    "    \"\"\"\n",
    "    Get equivariant basis by finding the subspace of symmetrized samples of (non-equivariant weights)\n",
    "    \n",
    "    return \n",
    "    - a basis in the form of a tensor stacking R tensors of shape (2 x 4), with R the rank of the basis.\n",
    "    - the rank of the basis (an integer)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "state_group_reps = get_cartpole_state_group_representations()\n",
    "action_group_reps = get_cartpole_action_group_representations()\n",
    "\n",
    "basis, rank = get_equivariant_basis(state_group_reps, action_group_reps, sample_size=500)\n",
    "\n",
    "print('basis:', basis)\n",
    "print('rank:', rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9755f2ff",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "- Have a look at the basis coefficients: what is the effect of the symmetrizer?\n",
    "- was the rank of the basis expected?\n",
    "\n",
    "Let's test this basis!\n",
    "Build some equivariant weights from this basis and test the resulting weights are indeed equivariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ef858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build some weights as a linear combination of the basis\n",
    "weights = # your code here\n",
    "\n",
    "\n",
    "# let's check if the associated network is indeed equivariant\n",
    "z = torch.tensor(np.random.rand(4,1).astype(np.float32))\n",
    "weights = torch.tensor(weights.astype(np.float32))\n",
    "\n",
    "test_network_is_equivariant(weights, z, group) # test should pass!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b096b84",
   "metadata": {},
   "source": [
    "## Going deep: Stacking equivariant layers\n",
    "\n",
    "If we want to go deep, we can stack layers $f(x) = f_2(f_1(x))$.\n",
    "\n",
    "How to do so to maintain the equivariance of the network?\n",
    "\n",
    "You can use intermediate transformation $L_g$:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align} \n",
    "K_g [f (x)] &= K_g [f_2(f_1(x))]\\\\\n",
    "&= f_2(P_g [f_1(x)]) \\quad \\text{($f_2$ equivariance constraint)}\\\\\n",
    "&= f_2(f_1(L_g [x])) \\quad \\text{($f_1$ equivariance constraint)}\\\\\n",
    "&= f (L_g [x]) \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "here we can use $L_g=K_g$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83547f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a deep equivariant network f(x) = f_2(f_1(x))\n",
    "# f_1(x) = sig(W_1 x + b_1)\n",
    "# f_2(x) = sig(W_2 x + b_2)\n",
    "\n",
    "# sample weights for layers 1 and 2\n",
    "W_1 = torch.tensor(np.random.rand(2, 4).astype(np.float32))\n",
    "W_2 = torch.tensor(np.random.rand(2, 2).astype(np.float32))\n",
    "# sample random state\n",
    "z = torch.tensor(np.random.rand(4,1).astype(np.float32))\n",
    "\n",
    "\n",
    "# get equivariant weights using the symmetrizer\n",
    "# we have already done the job for the first layer\n",
    "group_1 = # your code\n",
    "W_1_sym = # symmetrize weights W_1\n",
    "\n",
    "# for the second layer, the input/output transformations are different\n",
    "group_2 = # your code\n",
    "W_2_sym = # symmetrize weights W_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now checking that the resulting deep network is equivariant\n",
    "# I built f_2(f_1(x)) for you here.\n",
    "\n",
    "b_1 = .01\n",
    "b_2 = .01\n",
    "sig = lambda x: torch.relu( x)\n",
    "\n",
    "# compute f = f(x)\n",
    "h1 = sig(torch.matmul(W_1_sym, z) + b_1)\n",
    "f = sig(torch.matmul(W_2_sym, h1) + b_2)\n",
    "\n",
    "# compute f_p = f(L_g(x))\n",
    "L_g = group_1[0][1]\n",
    "zp = torch.matmul(L_g, z)\n",
    "h1p = sig(torch.matmul(W_1_sym, zp) + b_1) \n",
    "fp = sig(torch.matmul(W_2_sym, h1p) + b_2) \n",
    "\n",
    "# check that K_g f(L_g(x)) = f(x)\n",
    "K_g = group_1[1][1]\n",
    "all(torch.matmul(K_g, fp) == f) # test should pass!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd218d3",
   "metadata": {},
   "source": [
    "## BONUS: Training an equivariant policy\n",
    "\n",
    "We now have all the tools to build an equivariant policy!\n",
    "\n",
    "If you have code in pytorch to run PG, you can plug the policy architecture you obtained!\n",
    "\n",
    "\n",
    "Alternatively, you can expand the code base of Matteo with this particular policy class!\n",
    "\n",
    "\n",
    "\n",
    "Finally, you can use the code that came with the paper! https://github.com/ElisevanderPol/mdp-homomorphic-networks\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
