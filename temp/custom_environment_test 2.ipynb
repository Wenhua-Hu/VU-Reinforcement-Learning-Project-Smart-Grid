{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71BhW0cK9emd",
        "outputId": "92c0813b-4c4d-4cfe-dd9a-48aee698c766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box, MultiDiscrete\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gutXaPphnXyj"
      },
      "outputs": [],
      "source": [
        "# fix something here maybe in order to have it so that action is adjusted in step function to closest action that is allowed at 7am,\n",
        "# and so that the action does not result in negative battery level\n",
        "\n",
        "def get_legal_actions_at_last(all_actions, current_level, min_level=2):\n",
        "    '''\n",
        "    Expected behavior: [-2,-1,0,1,2], currently 0 -> [2]\n",
        "    Expected behavior: [-2,-1,0,1,2], currently 1 -> [1,2]\n",
        "    Expected behavior: [-2,-1,0,1,2], currently 2 -> [0,1,2]\n",
        "    Expected behavior: [-2,-1,0,1,2], currently 3 -> [-1,0,1,2]\n",
        "    Expected behavior: [-2,-1,0,1,2], currently 4 -> [-2,-1,0,1,2]\n",
        "    '''\n",
        "    all_actions = np.asarray(all_actions)\n",
        "    charge_needed = min_level - current_level\n",
        "    avail_actions = all_actions[np.where(all_actions >= charge_needed)]\n",
        "    # avail_actions = all_actions[list(np.where(all_actions >= charge_needed)[0])]\n",
        "    return avail_actions.tolist()\n",
        "\n",
        "def get_legal_actions(all_actions, current_level, min_level=0):\n",
        "    all_actions = np.asarray(all_actions)\n",
        "    underlying_levels = all_actions + current_level\n",
        "    # avail_actions = all_actions[list(np.where(underlying_levels >= 0)[0])]\n",
        "    avail_actions = all_actions[np.where(underlying_levels >= min_level)]\n",
        "    return avail_actions.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "t8HtS9kPM2qG"
      },
      "outputs": [],
      "source": [
        "# 1 make the step function go through one hour at a time, and through the entire dataframe (sequentially) in total *\n",
        "# 2 take at_home out of the state *\n",
        "# 3 include the action mechanics inside of the step function *\n",
        "# 4 test environment again on random policy loop *\n",
        "# 5 try q learning\n",
        "# 6 change action to -1, 1 range\n",
        "# 7 change reward calculation *\n",
        "# 8 test with the test environment (validate using main.py)\n",
        "# 9 include electricity price, and maybe more time elements in the state representation (week/month/year)\n",
        "# 10 change hour from 08-07 to 01-00\n",
        "\n",
        "#\n",
        "# action comes in in continuous form, is modified in the step function in continuous form within range [-1,1], which\n",
        "# is used then to update state variables, i.e. battery level, into a continuous variable between [0,50]\n",
        "# state is returned in continuous form\n",
        "# in tabular Q learning, state (from env.step) is taken and discretized to get next action\n",
        "# in approximator methods, state is taken as continuous and used to get next action\n",
        "\n",
        "class StorageEnv(Env):\n",
        "    def __init__(self, path_to_train_data):\n",
        "        '''\n",
        "        Initialization of the energy storage environment;\n",
        "        We interpret a run through all historical price data as one trajectory = one episode,\n",
        "        where for each hour of each day, we can run the step function to update from one state\n",
        "        to the next, given some action for that hour (the step function will return the next\n",
        "        state, as well as next reward, and whether the entire historical dataset has been\n",
        "        iterated through using the 'done' variable in the return statement).\n",
        "        '''\n",
        "\n",
        "        self.train_data = pd.read_excel(path_to_train_data)\n",
        "        self.price_values = self.train_data.iloc[:, 1:25].to_numpy()\n",
        "        self.timestamps = self.train_data['PRICES']\n",
        "        self.nr_hours = self.price_values.shape[0]*self.price_values.shape[1] # number of hours in the dataset in total = 25208 for train.xlsx\n",
        "\n",
        "\n",
        "        self.values_actions =  [-20, -10, 0, 10, 20]\n",
        "        self.batteries =  [0, 10, 20, 30, 40, 50]\n",
        "        self.action_repr = [-2,-1,0,1,2]\n",
        "\n",
        "        self.done = False # indicates whether trajectory (run through entire dataset) has finished, analogous to 'terminated' argument\n",
        "\n",
        "        self.action_space = Discrete(5, start=-2)\n",
        "        self.battery_space = Discrete(6, start=0)\n",
        "        self.position_space = Discrete(2, start=0)\n",
        "\n",
        "        self.counter = 0\n",
        "        self.hour = 1\n",
        "        self.day = 1\n",
        "        self.num_hours_day = 24\n",
        "\n",
        "        self.min_battery_level_start = 20\n",
        "        self.min_battery_level = 0\n",
        "        self.max_battery_level = 50\n",
        "        self.max_charging_level = 25\n",
        "\n",
        "        # battery level from 0 to 50 (10 incr.), indication car at home or not\n",
        "        self.observation_space = MultiDiscrete([self.battery_space.n,\n",
        "                                                self.position_space.n,\n",
        "                                                self.num_hours_day])\n",
        "\n",
        "        # initialize state vars\n",
        "        self.battery_level = np.random.randint(self.min_battery_level_start, self.max_battery_level)\n",
        "        # randomly initialize athome with 50/50 chance of being away at hour 0 (8am) or not\n",
        "        self.at_home = np.random.randint(self.position_space.n) # dont need this anymore or?\n",
        "        # initial state\n",
        "        self.state = [self.battery_level, self.hour]\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        ######### at current timestep t\n",
        "        ### battery comsumption at timestep t\n",
        "\n",
        "        # no charging/discharging if car is away, besides reduction of batter level by 20kwh at 8am (enforced later on)\n",
        "        if not self.at_home: # check if car is currently away, enforced by overriding previous if statement\n",
        "          action = 0\n",
        "\n",
        "        # meeting constraints of having 20kwh at 7am, and alwys more than 0kwh stored\n",
        "        elif self.at_home:\n",
        "          # calculate the exact action you need to get the battery to 20kwh, by taking the min of all sufficient (=legal) actions\n",
        "          if self.hour == 7:\n",
        "            legal_actions = get_legal_actions_at_last(self.action_repr, self.battery_level)\n",
        "            if action not in legal_actions: # check if action chosen by agent ensures 20kwh of battery at 7am\n",
        "              action = min(legal_actions)\n",
        "          # rest of the day, ensure that battery stays above 0kwh\n",
        "          else:\n",
        "            legal_actions = get_legal_actions(self.action_repr, self.battery_level)\n",
        "            if action not in legal_actions:\n",
        "              action = min(legal_actions)\n",
        "\n",
        "        # removing 20kwh from battery when going away from home\n",
        "        if not self.at_home:\n",
        "          self.battery_level = self.battery_level-2 if self.hour == 0 else self.battery_level\n",
        "\n",
        "        # update battery level based on the picked acion at timestep t\n",
        "        if self.at_home:\n",
        "            self.battery_level += action\n",
        "\n",
        "        # no over-charging above 50kwh\n",
        "        if self.battery_level >= 5:\n",
        "            self.battery_level = 5\n",
        "\n",
        "        # should we move the constraints not lower 0 here ??\n",
        "\n",
        "        ######### update the state for next hour, timestep t+1\n",
        "        # keep the hour 8 ~ 18 same, otherwise athome = True\n",
        "        self.at_home = self.at_home if self.hour < 18 else 1\n",
        "\n",
        "        # reward calculations\n",
        "        # if self.day-1 == 1095:\n",
        "        #   print(f\"self.day-1 {self.day-1} | self.hour-1 {self.hour-1}  | self.price_values[self.day-1][self.hour-1] {self.price_values[self.day-1][self.hour-1]}  \")\n",
        "        hourly_price = self.price_values[self.day-1][self.hour-1]\n",
        "        cost_factor = 1.0 if action < 0 else 2.0\n",
        "        efficiency_price_factor = 0.9 if action < 0 else 1.0 # obtained electricity is impacted by 0.9 when selling\n",
        "\n",
        "        # get amount of kwh bought during this step\n",
        "        kwhs_charged = action * 10 # change this 10 when changing action space!!!\n",
        "        price_of_charging = kwhs_charged * cost_factor * efficiency_price_factor * (hourly_price/1000) # go from MWh to KWh, multiply by 2 if we are buying\n",
        "        # add reward here, based on price from table\n",
        "\n",
        "        reward = (-1.) * price_of_charging # reward for this step, positive if selling, negative when buying\n",
        "\n",
        "        # update counter and time variables\n",
        "        self.counter += 1 # update continuous counter (running from 0 - len(df)*24)\n",
        "        self.hour += 1 # increment hour (running from 1-24 and back to 1 after the day passed)\n",
        "\n",
        "        if self.counter % 24 == 0: # check if day is over, meaning that it is midnight at timestep t+1\n",
        "            self.hour = 1 # reset hour of the day for next timestep\n",
        "            self.day += 1 # increment to start of next day\n",
        "\n",
        "        # update state\n",
        "        self.state = [self.battery_level, self.hour]\n",
        "\n",
        "        # check if all hours in the dataset have been seen\n",
        "        self.done = self.counter+1 == self.nr_hours\n",
        "\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        return self.state, reward, self.done, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "      # initialize battery level randomly between 0 and 6, representing the space 0-50kwh\n",
        "      self.battery_level = np.random.uniform(self.min_battery_level_start, self.max_battery_level)\n",
        "      # randomly initialize athome with 50/50 chance of being away at hour 0 (8am) or not\n",
        "      self.at_home = np.random.randint(self.position_space.n)\n",
        "      # hour may not need to be in the state..?\n",
        "      self.hour = 1\n",
        "      self.day = 1\n",
        "      self.counter = 0\n",
        "      self.done = False\n",
        "       # initial state\n",
        "      self.state = [self.battery_level, self.hour]\n",
        "\n",
        "      return self.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QAgent():\n",
        "    \n",
        "    def __init__(self, discount_rate = 0.95, bin_size = {'battery': 5, 'hour': 24}):\n",
        "        \n",
        "        '''\n",
        "        Params:\n",
        "        \n",
        "        env_name = name of the specific environment that the agent wants to solve\n",
        "        discount_rate = discount rate used for future rewards\n",
        "        bin_size = number of bins used for discretizing the state space\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        #create an environment\n",
        "        self.env = StorageEnv(path_to_train_data=\"../data/train.xlsx\")\n",
        "        \n",
        "        #Set the discount rate\n",
        "        self.discount_rate = discount_rate\n",
        "        \n",
        "        #The algorithm has then 3 different actions\n",
        "        #0: Accelerate to the left\n",
        "        #1: Don't accelerate\n",
        "        #2: Accelerate to the right\n",
        "        self.action_space = self.env.action_space.n\n",
        "        self.action_repr = [-2,-1,0,1,2]\n",
        "        \n",
        "        #Set the bin size\n",
        "        self.bin_size = bin_size\n",
        "        \n",
        "        # print(self.action_space, self.bin_size)\n",
        "        \n",
        "        #State incorporates the observation state\n",
        "        #State[0] is x position\n",
        "        #State[1] is velocity\n",
        "    \n",
        "        #Get the low and high values of the environment space\n",
        "        # self.low = self.env.observation_space.low\n",
        "        # self.high = self.env.observation_space.high\n",
        "        \n",
        "        \n",
        "        self.battery_low = 0\n",
        "        self.battery_high = 50\n",
        "        self.hour_low = 0\n",
        "        self.hour_high = 24\n",
        "        #Create bins for both observation features, i.e. x-position and velocity\n",
        "\n",
        "        # self.bin_x = np.linspace(self.low[0], self.high[0], self.bin_size)\n",
        "        self.bin_battery = np.linspace(self.battery_low, self.battery_high, self.bin_size['battery'] + 1)  # might add 2 : 0, 50  ??  7\n",
        "        # print(self.bin_battery)\n",
        "\n",
        "        # print(f\"self.bin_x:  {self.low[0]} | {self.high[0]} | {self.bin_size} |{self.bin_x }\")\n",
        "    \n",
        "        '''\n",
        "        ToDo:\n",
        "        \n",
        "        Please create the bins for the velocity feature in the same manner and call this variable self.bin_velocity!\n",
        "        '''\n",
        "                \n",
        "        #Solution\n",
        "        # self.bin_velocity = np.linspace(self.low[1], self.high[1], self.bin_size)\n",
        "        # array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.])\n",
        "        self.bin_hour = np.linspace(0, 24, self.bin_size['hour'] + 1 )  # 0-1 -> 1   24\n",
        "        # print(self.bin_hour)\n",
        "        \n",
        "        #Append the two bins\n",
        "        # self.bins = [self.bin_x, self.bin_velocity]\n",
        "        self.bins = [self.bin_battery, self.bin_hour]\n",
        "        \n",
        "    \n",
        "    def discretize_state(self, state):\n",
        "        \n",
        "        '''\n",
        "        Params:\n",
        "        state = state observation that needs to be discretized\n",
        "        \n",
        "        \n",
        "        Returns:\n",
        "        discretized state\n",
        "        '''\n",
        "        #Now we can make use of the function np.digitize and bin it\n",
        "        self.state = state\n",
        "        # print(f\"self.state: {self.state}\")\n",
        "        \n",
        "        #Create an empty state\n",
        "        digitized_state = []\n",
        "    \n",
        "        # battery -> bin0: -inf~0 bin1: 0~10, ... bin6: 50~inf   \n",
        "        # print(f\"np.digitize(self.state[0], self.bins[0]): {np.digitize(self.state[0], self.bins[0])}\")\n",
        "        # print(f\"battery: {self.state[0]}, bins: {self.bins[0]}\")\n",
        "        # print(f\"at which bin: {np.digitize(self.state[0], self.bins[0])}\")\n",
        "        digitized_state.append(np.digitize(self.state[0], self.bins[0], right=False)-1)  # battery ??   0, 1, 2, 3, 4, 5   from [-inf, 0) is not considered\n",
        "        # hour -> bin0: 0~1 bin1: 1~2, ... bin23: 23~24, bin24: 24-inf (never reach)  \n",
        "        # print(f\"np.digitize(self.state[1], self.bin_hour[1]): {np.digitize(self.state[1], self.bins[1])}\")\n",
        "        \n",
        "        # self.state[1] = 1\n",
        "        # hour: 0.5, bins: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24.]   .->\n",
        "        # print(f\"hour: {self.state[1]}, bins: {self.bins[1]}\")\n",
        "        # print(f\"at which bin: {np.digitize(self.state[1], self.bins[1], right=True)}\") \n",
        "        digitized_state.append(np.digitize(self.state[1], self.bins[1], right=True)-1)  # hour   0, 1, 2 ... 23, 24   from [-inf,0]  is not considered\n",
        "        \n",
        "        # for i in range(len(self.bins)):\n",
        "        #     # print(f\"self.state[i], self.bins[i]: {self.state[i]}, {self.bins[i]}\")\n",
        "        #     # print(f\"np.digitize(self.state[i], self.bins[i]): {np.digitize(self.state[i], self.bins[i])}\")\n",
        "        #     digitized_state.append(np.digitize(self.state[i], self.bins[i])-1)  ##\n",
        "        #     # print(f\"np.digitize(self.state[i], self.bins[i])-1: {np.digitize(self.state[i], self.bins[i])-1}\")\n",
        "        \n",
        "        #Returns the discretized state from an observation\n",
        "        return digitized_state\n",
        "    \n",
        "    def discretize_action(self, action=1.0):\n",
        "        digitized_action = action\n",
        "        return digitized_action\n",
        "    \n",
        "    def create_Q_table(self):\n",
        "        # self.state_space = self.bin_size - 1\n",
        "        #Initialize all values in the Q-table to zero\n",
        "        self.state_battery_space = self.bin_size['battery']\n",
        "        self.state_hour_space = self.bin_size['hour']\n",
        "        \n",
        "        '''\n",
        "        ToDo:\n",
        "        Initialize a zero matrix of dimension state_space * state_space * action_space and call it self.Qtable!\n",
        "        '''\n",
        "        \n",
        "        #Solution:\n",
        "        # self.Qtable = np.zeros((self.state_space, self.state_space, self.action_space))\n",
        "        self.Qtable = np.zeros((self.state_battery_space, self.state_hour_space, self.action_space))\n",
        "        # print(self.Qtable.shape)\n",
        "        \n",
        "\n",
        "    def train(self, simulations, learning_rate, epsilon = 0.05, epsilon_decay = 1000, adaptive_epsilon = False, \n",
        "              adapting_learning_rate = False):\n",
        "        \n",
        "        '''\n",
        "        Params:\n",
        "        \n",
        "        simulations = number of episodes of a game to run\n",
        "        learning_rate = learning rate for the update equation\n",
        "        epsilon = epsilon value for epsilon-greedy algorithm\n",
        "        epsilon_decay = number of full episodes (games) over which the epsilon value will decay to its final value\n",
        "        adaptive_epsilon = boolean that indicates if the epsilon rate will decay over time or not\n",
        "        adapting_learning_rate = boolean that indicates if the learning rate should be adaptive or not\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        #Initialize variables that keep track of the rewards\n",
        "        \n",
        "        self.rewards = []\n",
        "        self.average_rewards = []\n",
        "        \n",
        "        #Call the Q table function to create an initialized Q table\n",
        "        self.create_Q_table()\n",
        "        \n",
        "        #Set epsilon rate, epsilon decay and learning rate\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        #Set start epsilon, so here we want a starting exploration rate of 1\n",
        "        self.epsilon_start = 1\n",
        "        self.epsilon_end = 0.05\n",
        "        \n",
        "        #If we choose adaptive learning rate, we start with a value of 1 and decay it over time!\n",
        "        if adapting_learning_rate:\n",
        "            self.learning_rate = 1\n",
        "        \n",
        "        for i in range(simulations):\n",
        "            \n",
        "            if i % 5000 == 0:\n",
        "                print(f'Please wait, the algorithm is learning! The current simulation is {i}')\n",
        "            # print(f'Please wait, the algorithm is learning! The current simulation is {i}')           \n",
        "            #Initialize the state\n",
        "            # state = self.env.reset()[0]   # reset returns a dict, need to take the 0th entry.\n",
        "            state = self.env.reset()   # reset returns a dict, need to take the 0th entry.\n",
        "        \n",
        "            #Set a variable that flags if an episode has terminated\n",
        "            done = False\n",
        "        \n",
        "            #Discretize the state space\n",
        "            # print(f\"state: {state}\")\n",
        "            state = self.discretize_state(state)\n",
        "            # print(f\"discretize_state state: {state}\")\n",
        "            \n",
        "            \n",
        "            #Set the rewards to 0\n",
        "            total_rewards = 0\n",
        "            \n",
        "            #If adaptive epsilon rate\n",
        "            # self.epsilon_start = 1\n",
        "            # self.epsilon_end = 0.05\n",
        "            # epsilon = 0.05\n",
        "            # epsilon_decay = 1000\n",
        "            if adaptive_epsilon:\n",
        "                self.epsilon = np.interp(i, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
        "                \n",
        "                #Logging just to check it decays as we want it to do, we just print out the first three statements\n",
        "                if i % 500 == 0 and i <= 1500:\n",
        "                    print(f\"The current epsilon rate is {self.epsilon}\")\n",
        "                \n",
        "            #Loop until an episode has terminated\n",
        "            while not done:\n",
        "                \n",
        "                #Pick an action based on epsilon greedy\n",
        "                \n",
        "                '''\n",
        "                ToDo: Write the if statement that picks a random action\n",
        "                Tip: Make use of np.random.uniform() and the self.epsilon to make a decision!\n",
        "                Tip: You can also make use of the method sample() of the self.env.action_space \n",
        "                    to generate a random action!\n",
        "                '''\n",
        "                \n",
        "                #Solution:\n",
        "                \n",
        "                #Pick random action\n",
        "                if np.random.uniform(0, 1) > 1-self.epsilon:\n",
        "                    #This picks a random action from 0,1,2\n",
        "                    action = self.env.action_space.sample()\n",
        "                    \n",
        "                #Pick a greedy action\n",
        "                else:\n",
        "                    action = np.argmax(self.Qtable[state[0],state[1],:])\n",
        "                    \n",
        "                #Now sample the next_state, reward, done and info from the environment\n",
        "                \n",
        "                next_state, reward, terminated, truncated, info = self.env.step(action) # step returns 5 outputs\n",
        "                done =  terminated or truncated\n",
        "                \n",
        "                #Now discretize the next_state\n",
        "                next_state = self.discretize_state(next_state)\n",
        "                \n",
        "                #Target value \n",
        "                Q_target = (reward + self.discount_rate*np.max(self.Qtable[next_state[0], next_state[1]]))\n",
        "                \n",
        "                #Calculate the Temporal difference error (delta)\n",
        "                delta = self.learning_rate * (Q_target - self.Qtable[state[0], state[1], action])\n",
        "                \n",
        "                #Update the Q-value\n",
        "                self.Qtable[state[0], state[1], action] = self.Qtable[state[0], state[1], action] + delta\n",
        "                \n",
        "                #Update the reward and the hyperparameters\n",
        "                total_rewards += reward\n",
        "                state = next_state\n",
        "                \n",
        "            \n",
        "            if adapting_learning_rate:\n",
        "                self.learning_rate = self.learning_rate/np.sqrt(i+1)\n",
        "            \n",
        "            self.rewards.append(total_rewards)\n",
        "            \n",
        "            #Calculate the average score over 100 episodes\n",
        "            if i % 100 == 0:\n",
        "                self.average_rewards.append(np.mean(self.rewards))\n",
        "                \n",
        "                #Initialize a new reward list, as otherwise the average values would reflect all rewards!\n",
        "                self.rewards = []\n",
        "        \n",
        "        print('The simulation is done!')\n",
        "        \n",
        "    def visualize_rewards(self):\n",
        "        plt.figure(figsize =(7.5,7.5))\n",
        "        plt.plot(100*(np.arange(len(self.average_rewards))+1), self.average_rewards)\n",
        "        plt.axhline(y = -110, color = 'r', linestyle = '-')\n",
        "        plt.title('Average reward over the past 100 simulations', fontsize = 10)\n",
        "        plt.legend(['Q-learning performance','Benchmark'])\n",
        "        plt.xlabel('Number of simulations', fontsize = 10)\n",
        "        plt.ylabel('Average reward', fontsize = 10)\n",
        "            \n",
        "    def play_game(self):\n",
        "        # Make eval env which renders when taking a step\n",
        "        # eval_env = gym.make(env_name, render_mode='human')\n",
        "        eval_env = StorageEnv(path_to_train_data=\"../data/validate.xlsx\")\n",
        "        state = eval_env.reset()\n",
        "        done=False\n",
        "        # Run the environment for 1 episode\n",
        "        while not done:\n",
        "            state = self.discretize_state(state)\n",
        "            action = np.argmax(self.Qtable[state[0],state[1],:])\n",
        "            next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "        eval_env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please wait, the algorithm is learning! The current simulation is 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[81], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#WRITE YOUR CODE HERE!\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#With standard epsilon_greedy\u001b[39;00m\n\u001b[0;32m     13\u001b[0m agent_standard_greedy \u001b[38;5;241m=\u001b[39m QAgent()\n\u001b[1;32m---> 14\u001b[0m \u001b[43magent_standard_greedy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[80], line 229\u001b[0m, in \u001b[0;36mQAgent.train\u001b[1;34m(self, simulations, learning_rate, epsilon, epsilon_decay, adaptive_epsilon, adapting_learning_rate)\u001b[0m\n\u001b[0;32m    226\u001b[0m done \u001b[38;5;241m=\u001b[39m  terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m#Now discretize the next_state\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m#Target value \u001b[39;00m\n\u001b[0;32m    232\u001b[0m Q_target \u001b[38;5;241m=\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_rate\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQtable[next_state[\u001b[38;5;241m0\u001b[39m], next_state[\u001b[38;5;241m1\u001b[39m]]))\n",
            "Cell \u001b[1;32mIn[80], line 91\u001b[0m, in \u001b[0;36mQAgent.discretize_state\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     85\u001b[0m digitized_state \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# battery -> bin0: -inf~0 bin1: 0~10, ... bin6: 50~inf   \u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# print(f\"np.digitize(self.state[0], self.bins[0]): {np.digitize(self.state[0], self.bins[0])}\")\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# print(f\"battery: {self.state[0]}, bins: {self.bins[0]}\")\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# print(f\"at which bin: {np.digitize(self.state[0], self.bins[0])}\")\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m digitized_state\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# battery ??   0, 1, 2, 3, 4, 5   from [-inf, 0) is not considered\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# hour -> bin0: 0~1 bin1: 1~2, ... bin23: 23~24, bin24: 24-inf (never reach)  \u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# print(f\"np.digitize(self.state[1], self.bin_hour[1]): {np.digitize(self.state[1], self.bins[1])}\")\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# print(f\"hour: {self.state[1]}, bins: {self.bins[1]}\")\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# print(f\"at which bin: {np.digitize(self.state[1], self.bins[1], right=True)}\") \u001b[39;00m\n\u001b[0;32m     99\u001b[0m digitized_state\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mdigitize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins[\u001b[38;5;241m1\u001b[39m], right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# hour   0, 1, 2 ... 23, 24   from [-inf,0]  is not considered\u001b[39;00m\n",
            "File \u001b[1;32me:\\Users\\Linuxer\\miniconda3\\envs\\rl_env\\Lib\\site-packages\\numpy\\lib\\function_base.py:5732\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   5731\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\Users\\Linuxer\\miniconda3\\envs\\rl_env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1400\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\Users\\Linuxer\\miniconda3\\envs\\rl_env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#You can change the learning rate and the number of simulations if you want (yet, it will take then of course longer)!\n",
        "simulations = 15000\n",
        "learning_rate = 0.10\n",
        "\n",
        "'''\n",
        "ToDo:\n",
        "Initialize the Qagent and call the instance agent!\n",
        "Afterwards,let it train over the number of simulations with the specific learning rate!\n",
        "'''\n",
        "\n",
        "#WRITE YOUR CODE HERE!\n",
        "#With standard epsilon_greedy\n",
        "agent_standard_greedy = QAgent()\n",
        "agent_standard_greedy.train(simulations, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "LqIGGN-lEXIt",
        "outputId": "694f4a75-51c5-4bc4-f3b2-34aed74c14f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At episode 1, total electricity expenditure is: -10783.128680000014\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1931fbda9d0>]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# env = StorageEnv(\"../data/train.xlsx\")\n",
        "\n",
        "# total_cost = 0.0\n",
        "# action_repr = [-2,-1,0,1,2]\n",
        "# battery_charges = []\n",
        "\n",
        "# for episode in range(1):\n",
        "#     # current_day_prices = prices.iloc[episode].tolist()\n",
        "#     # env.reset() # fix the reset function and uncomment when running >1 episodes\n",
        "#     battery_level, hour = env.state\n",
        "#     done=False\n",
        "\n",
        "#     while not done:\n",
        "#       action = random.choice(action_repr)\n",
        "#       # print(env.counter) # counter goes until 26302 = 25208 + 1096, not sure why\n",
        "#       state, reward, done, truncated, info = env.step(action) # get the new state\n",
        "#       battery_level, hour = state\n",
        "\n",
        "#       battery_charges.append(battery_level)\n",
        "\n",
        "#       total_cost += reward\n",
        "\n",
        "#     # finished, all hours*days have been seen\n",
        "#     print(f\"At episode {episode+1}, total electricity expenditure is: {total_cost}\")\n",
        "\n",
        "# # restrict actions for selling\n",
        "\n",
        "# plt.plot(battery_charges)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
