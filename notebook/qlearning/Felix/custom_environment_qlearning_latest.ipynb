{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71BhW0cK9emd",
        "outputId": "92c0813b-4c4d-4cfe-dd9a-48aee698c766"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box, MultiDiscrete, Tuple\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import gym\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gutXaPphnXyj"
      },
      "outputs": [],
      "source": [
        "# # fix something here maybe in order to have it so that action is adjusted in step function to closest action that is allowed at 7am,\n",
        "# # and so that the action does not result in negative battery level\n",
        "\n",
        "# def get_legal_actions_at_last(all_actions, current_level, min_level=2):\n",
        "#     '''\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 0 -> [2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 1 -> [1,2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 2 -> [0,1,2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 3 -> [-1,0,1,2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 4 -> [-2,-1,0,1,2]\n",
        "#     '''\n",
        "#     all_actions = np.asarray(all_actions)\n",
        "#     charge_needed = min_level - current_level\n",
        "#     avail_actions = all_actions[np.where(all_actions >= charge_needed)]\n",
        "#     # avail_actions = all_actions[list(np.where(all_actions >= charge_needed)[0])]\n",
        "#     return avail_actions.tolist()\n",
        "\n",
        "# def get_legal_actions(all_actions, current_level, min_level=0):\n",
        "#     all_actions = np.asarray(all_actions)sim\n",
        "#     underlying_levels = all_actions + current_level\n",
        "#     # avail_actions = all_actions[list(np.where(underlying_levels >= 0)[0])]\n",
        "#     avail_actions = all_actions[np.where(underlying_levels >= min_level)]\n",
        "#     return avail_actions.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fix something here maybe in order to have it so that action is adjusted in step function to closest action that is allowed at 7am,\n",
        "# and so that the action does not result in negative battery level\n",
        "\n",
        "# def get_legal_actions_at_last(all_actions, current_level, min_level=2):\n",
        "#     '''\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 0 -> [2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 1 -> [1,2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 2 -> [0,1,2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 3 -> [-1,0,1,2]\n",
        "#     Expected behavior: [-2,-1,0,1,2], currently 4 -> [-2,-1,0,1,2]\n",
        "#     '''\n",
        "#     all_actions = np.asarray(all_actions)\n",
        "#     charge_needed = min_level - current_level\n",
        "#     avail_actions = all_actions[np.where(all_actions >= charge_needed)]\n",
        "#     # avail_actions = all_actions[list(np.where(all_actions >= charge_needed)[0])]\n",
        "#     return avail_actions.tolist()\n",
        "\n",
        "# def get_legal_actions(all_actions, current_level, min_level=0):\n",
        "#     all_actions = np.asarray(all_actions)\n",
        "#     underlying_levels = all_actions + current_level\n",
        "#     # avail_actions = all_actions[list(np.where(underlying_levels >= 0)[0])]\n",
        "#     avail_actions = all_actions[np.where(underlying_levels >= min_level)]\n",
        "#     return avail_actions.tolist()\n",
        "\n",
        "\n",
        "# def get_legal_range(full_range, current_level, min_level=20):\n",
        "#     '''\n",
        "#     Expected behavior: [-25,25], currently 0 -> [20,25]\n",
        "#     Expected behavior: [-25,25], currently 10 -> [10,25]\n",
        "#     Expected behavior: [-25,25], currently 20-> [0,25]\n",
        "#     Expected behavior: [-25,25], currently 30-> [-10,25]\n",
        "#     Expected behavior: [-25,25], currently 40 -> [-20,25]\n",
        "#     Expected behavior: [-25,25], currently 50 -> [-25,25]\n",
        "#     '''\n",
        "#     full_range = np.asarray(full_range)\n",
        "#     charge_needed = min_level - current_level\n",
        "#     avail_actions = [charge_needed, 25]\n",
        "#     return avail_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t8HtS9kPM2qG"
      },
      "outputs": [],
      "source": [
        "# # 1 make the step function go through one hour at a time, and through the entire dataframe (sequentially) in total *\n",
        "# # 2 take at_home out of the state *\n",
        "# # 3 include the action mechanics inside of the step function *\n",
        "# # 4 test environment again on random policy loop *\n",
        "# # 5 try q learning\n",
        "# # 6 change action to -1, 1 range\n",
        "# # 7 change reward calculation *\n",
        "# # 8 test with the test environment (validate using main.py)\n",
        "# # 9 include electricity price, and maybe more time elements in the state representation (week/month/year)\n",
        "# # 10 change hour from 08-07 to 01-00\n",
        "\n",
        "# #\n",
        "# # action comes in in continuous form, is modified in the step function in continuous form within range [-1,1], which\n",
        "# # is used then to update state variables, i.e. battery level, into a continuous variable between [0,50]\n",
        "# # state is returned in continuous form\n",
        "# # in tabular Q learning, state (from env.step) is taken and discretized to get next action\n",
        "# # in approximator methods, state is taken as continuous and used to get next action\n",
        "\n",
        "# class StorageEnv(Env):\n",
        "#     def __init__(self, path_to_train_data):\n",
        "#         '''\n",
        "#         Initialization of the energy storage environment;\n",
        "#         We interpret a run through all historical price data as one trajectory = one episode,\n",
        "#         where for each hour of each day, we can run the step function to update from one state\n",
        "#         to the next, given some action for that hour (the step function will return the next\n",
        "#         state, as well as next reward, and whether the entire historical dataset has been\n",
        "#         iterated through using the 'done' variable in the return statement).\n",
        "#         '''\n",
        "\n",
        "#         self.train_data = pd.read_excel(path_to_train_data)\n",
        "#         self.price_values = self.train_data.iloc[:, 1:25].to_numpy()\n",
        "#         self.timestamps = self.train_data['PRICES']\n",
        "#         self.nr_hours = self.price_values.shape[0]*self.price_values.shape[1] # number of hours in the dataset in total = 25208 for train.xlsx\n",
        "\n",
        "\n",
        "#         self.values_actions =  [-20, -10, 0, 10, 20]\n",
        "#         self.batteries =  [0, 10, 20, 30, 40, 50]\n",
        "#         self.action_repr = [-2,-1,0,1,2]\n",
        "\n",
        "#         self.done = False # indicates whether trajectory (run through entire dataset) has finished, analogous to 'terminated' argument\n",
        "\n",
        "#         self.action_space = Discrete(5, start=-2)\n",
        "#         self.battery_space = Discrete(6, start=0)\n",
        "#         self.position_space = Discrete(2, start=0)\n",
        "\n",
        "#         self.counter = 0\n",
        "#         self.hour = 1\n",
        "#         self.day = 1\n",
        "#         self.num_hours_day = 24\n",
        "\n",
        "#         self.min_battery_level_start = 20\n",
        "#         self.min_battery_level = 0\n",
        "#         self.max_battery_level = 50\n",
        "#         self.max_charging_level = 25\n",
        "\n",
        "#         # battery level from 0 to 50 (10 incr.), indication car at home or not\n",
        "#         self.observation_space = MultiDiscrete([self.battery_space.n,\n",
        "#                                                 self.position_space.n,\n",
        "#                                                 self.num_hours_day])\n",
        "\n",
        "#         # initialize state vars\n",
        "#         self.battery_level = np.random.randint(self.min_battery_level_start, self.max_battery_level)\n",
        "#         # randomly initialize athome with 50/50 chance of being away at hour 0 (8am) or not\n",
        "#         self.at_home = np.random.randint(self.position_space.n) # dont need this anymore or?\n",
        "#         # initial state\n",
        "#         self.state = [self.battery_level, self.hour]\n",
        "\n",
        "\n",
        "#     def step(self, action):\n",
        "#         ######### at current timestep t\n",
        "#         ### battery comsumption at timestep t\n",
        "\n",
        "#         # no charging/discharging if car is away, besides reduction of batter level by 20kwh at 8am (enforced later on)\n",
        "#         if not self.at_home: # check if car is currently away, enforced by overriding previous if statement\n",
        "#           action = 0\n",
        "\n",
        "#         # meeting constraints of having 20kwh at 7am, and alwys more than 0kwh stored\n",
        "#         elif self.at_home:\n",
        "#           # calculate the exact action you need to get the battery to 20kwh, by taking the min of all sufficient (=legal) actions\n",
        "#           if self.hour == 7:\n",
        "#             legal_actions = get_legal_actions_at_last(self.action_repr, self.battery_level)\n",
        "#             if action not in legal_actions: # check if action chosen by agent ensures 20kwh of battery at 7am\n",
        "#               action = min(legal_actions)\n",
        "#           # rest of the day, ensure that battery stays above 0kwh\n",
        "#           else:\n",
        "#             legal_actions = get_legal_actions(self.action_repr, self.battery_level)\n",
        "#             if action not in legal_actions:\n",
        "#               action = min(legal_actions)\n",
        "\n",
        "#         # removing 20kwh from battery when going away from home\n",
        "#         if not self.at_home:\n",
        "#           self.battery_level = self.battery_level-2 if self.hour == 0 else self.battery_level\n",
        "\n",
        "#         # update battery level based on the picked acion at timestep t\n",
        "#         if self.at_home:\n",
        "#             self.battery_level += action\n",
        "\n",
        "#         # no over-charging above 50kwh\n",
        "#         if self.battery_level >= 5:\n",
        "#             self.battery_level = 5\n",
        "\n",
        "#         # should we move the constraints not lower 0 here ??\n",
        "\n",
        "#         ######### update the state for next hour, timestep t+1\n",
        "#         # keep the hour 8 ~ 18 same, otherwise athome = True\n",
        "#         self.at_home = self.at_home if self.hour < 18 else 1\n",
        "\n",
        "#         # reward calculations\n",
        "#         # if self.day-1 == 1095:\n",
        "#         #   print(f\"self.day-1 {self.day-1} | self.hour-1 {self.hour-1}  | self.price_values[self.day-1][self.hour-1] {self.price_values[self.day-1][self.hour-1]}  \")\n",
        "#         hourly_price = self.price_values[self.day-1][self.hour-1]\n",
        "#         cost_factor = 1.0 if action < 0 else 2.0\n",
        "#         efficiency_price_factor = 0.9 if action < 0 else 1.0 # obtained electricity is impacted by 0.9 when selling\n",
        "\n",
        "#         # get amount of kwh bought during this step\n",
        "#         kwhs_charged = action * 10 # change this 10 when changing action space!!!\n",
        "#         price_of_charging = kwhs_charged * cost_factor * efficiency_price_factor * (hourly_price/1000) # go from MWh to KWh, multiply by 2 if we are buying\n",
        "#         # add reward here, based on price from table\n",
        "\n",
        "#         reward = (-1.) * price_of_charging # reward for this step, positive if selling, negative when buying\n",
        "\n",
        "#         # update counter and time variables\n",
        "#         self.counter += 1 # update continuous counter (running from 0 - len(df)*24)\n",
        "#         self.hour += 1 # increment hour (running from 1-24 and back to 1 after the day passed)\n",
        "\n",
        "#         if self.counter % 24 == 0: # check if day is over, meaning that it is midnight at timestep t+1\n",
        "#             self.hour = 1 # reset hour of the day for next timestep\n",
        "#             self.day += 1 # increment to start of next day\n",
        "\n",
        "#         # update state\n",
        "#         self.state = [self.battery_level, self.hour]\n",
        "\n",
        "#         # check if all hours in the dataset have been seen\n",
        "#         self.done = self.counter+1 == self.nr_hours\n",
        "\n",
        "#         truncated = False\n",
        "#         info = {}\n",
        "\n",
        "#         return self.state, reward, self.done, truncated, info\n",
        "\n",
        "#     def render(self):\n",
        "#         pass\n",
        "\n",
        "#     def reset(self):\n",
        "#       # initialize battery level randomly between 0 and 6, representing the space 0-50kwh\n",
        "#       self.battery_level = np.random.uniform(self.min_battery_level_start, self.max_battery_level)\n",
        "#       # randomly initialize athome with 50/50 chance of being away at hour 0 (8am) or not\n",
        "#       self.at_home = np.random.randint(self.position_space.n)\n",
        "#       # hour may not need to be in the state..?\n",
        "#       self.hour = 1\n",
        "#       self.day = 1\n",
        "#       self.counter = 0\n",
        "#       self.done = False\n",
        "#        # initial state\n",
        "#       self.state = [self.battery_level, self.hour]\n",
        "\n",
        "#       return self.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 make the step function go through one hour at a time, and through the entire dataframe (sequentially) in total *\n",
        "# 2 take at_home out of the state *\n",
        "# 3 include the action mechanics inside of the step function *\n",
        "# 4 test environment again on random policy loop *\n",
        "# 5 try q learning\n",
        "# 6 change action to -1, 1 range *\n",
        "# 7 change reward calculation *\n",
        "# 8 test with the test environment (validate using main.py)\n",
        "# 9 include electricity price, and maybe more time elements in the state representation (week/month/year)\n",
        "# 10 change hour from 08-07 to 01-00 *\n",
        "\n",
        "### contuinuous action mechanism:\n",
        "# action comes in in continuous form in training loop, is modified in the step function in continuous form within range [-1,1], which\n",
        "# is used then to update state variables, i.e. battery level, into a continuous variable between [0,50]\n",
        "# state is returned in continuous form\n",
        "# in tabular Q learning, state (from env.step) is taken and discretized to get next action\n",
        "# in approximator methods, state is taken as continuous and used to get next action\n",
        "\n",
        "class StorageEnv(gym.Env):\n",
        "    def __init__(self, path_to_train_data, num_actions=3, penalty_on=True):\n",
        "        '''\n",
        "        Initialization of the energy storage environment;\n",
        "        We interpret a run through all historical price data as one trajectory = one episode,\n",
        "        where for each hour of each day, we can run the step function to update from one state\n",
        "        to the next, given some action for that hour (the step function will return the next\n",
        "        state, as well as next reward, and whether the entire historical dataset has been\n",
        "        iterated through using the 'done' variable in the return statement).\n",
        "        '''\n",
        "\n",
        "        # check if penalties are enabled or not\n",
        "        self.penalty_on = penalty_on\n",
        "\n",
        "        self.train_data = pd.read_excel(path_to_train_data)\n",
        "        self.price_values = self.train_data.iloc[:, 1:25].to_numpy()\n",
        "        self.timestamps = self.train_data['PRICES']\n",
        "        self.state = np.empty(3)\n",
        "        # self.nr_hours = self.price_values.shape[0]*self.price_values.shape[1] # number of hours in the dataset in total = 25208 for train.xlsx\n",
        "        self.nr_hours = np.size(self.price_values)\n",
        "        # print(self.nr_hours)\n",
        "\n",
        "        self.battery_range = Box(low=0, high=50, shape=(1,), dtype=np.float32)\n",
        "        # print(self.price_values.max())\n",
        "        self.price_range = Box(low=0, high=self.price_values.max(), shape=(1,), dtype=np.float32)\n",
        "        # self.hour_range = Discrete(24, start=1)\n",
        "\n",
        "        # obv space: battery level from 0 to 50 (continuous), hour of the dayt\n",
        "        # self.observation_space = Tuple((self.battery_range, self.hour_range))\n",
        "        self.position_range = Discrete(2)\n",
        "        # self.dow_range = Discrete(7)\n",
        "        # self.month_range = Discrete(12, start=1)\n",
        "        # obv space: battery level from 0 to 50 (continuous), hour of the dayt\n",
        "        # self.observation_space = Tuple((self.battery_range, self.price_range, self.hour_range ))\n",
        "\n",
        "        # self.action_space = np.array([i for i in range(21)]) / 10.0 - 1.0\n",
        "        self.action_spaces = {\n",
        "            '3': np.array([-1, 0, 1]),\n",
        "            '5': np.array([-1, -0.5, 0, 0.5, 1]),\n",
        "            '11': np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0 ]),\n",
        "            '21': np.array([-1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0 , 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1.0 ])\n",
        "        }\n",
        "        self.action_space = self.action_spaces[str(int(num_actions))]\n",
        "\n",
        "        # print(self.action_space)\n",
        "        # self.actions = Discrete(len(self.action_repr))\n",
        "        self.action_space_n = len(self.action_space)\n",
        "        print(f\"Action space ({self.action_space_n} actions): {self.action_space}\")\n",
        "        # self.action_space = Discrete(21, start=-2) # -1 -0.9 8 7 6 5 4 3 2 1 0 1 2 \n",
        "        # self.cont_action_space = Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
        "        # self.action_repr = [-2,-1,0,1,2]\n",
        "\n",
        "        # self.position_space = Discrete(2, start=0)\n",
        "\n",
        "        # self.done = False # indicates whether trajectory (run through entire dataset) has finished, analogous to 'terminated' argument\n",
        "        \n",
        "        # self.min_battery_level_start = 20  #minimum_morning_level\n",
        "        # self.min_battery_level = 0\n",
        "        # self.max_battery_level = 50   #battery_capacity\n",
        "        # self.max_charging_level = 25\n",
        "        \n",
        "        # Battery characteristics\n",
        "        self.battery_capacity = 50  # kWh\n",
        "        self.max_power = 25 / 0.9  # kW\n",
        "        self.charge_efficiency = 0.9  # -\n",
        "        self.discharge_efficiency = 0.9  # -\n",
        "        # self.battery_level = self.battery_capacity / 2  # kWh (start at 50%)\n",
        "        self.minimum_morning_level = 20  # kWh\n",
        "        self.car_use_consumption = 20  # kWh\n",
        "\n",
        "        # Time Tracking\n",
        "        # self.counter = 0\n",
        "        # self.hour = 1\n",
        "        # self.day = 1\n",
        "        # self.car_is_available = True\n",
        "\n",
        "        # initialize state vars\n",
        "        # self.battery_level = np.random.uniform(self.min_battery_level_start, self.max_battery_level) # continuous value between 20 and 50\n",
        "        # randomly initialize athome with 50/50 chance of being away at hour 0 (8am) or not\n",
        "        # self.at_home = np.random.randint(self.position_space.n) # 50% chance at home at start of each day\n",
        "        # initial state\n",
        "        # self.state = np.array([self.battery_level, self.hour])\n",
        "\n",
        "\n",
        "    def step(self, action): # action is between -1 and 1\n",
        "        action = np.squeeze(action)\n",
        "        # print(action)\n",
        "        ######### at current timestep t\n",
        "        ### battery comsumption at timestep t\n",
        "        \n",
        "        if action <-1 or action >1:\n",
        "            raise ValueError('Action must be between -1 and 1')\n",
        "        \n",
        "        # store the action into action_bs\n",
        "        action_bs = action\n",
        "        # Calculate if, at 7am and after the chosen action, the battery level will be below the minimum morning level:\n",
        "        penalty = 0.0 # peality\n",
        "        if self.hour == 7:\n",
        "            if action > 0 and (self.battery_level < self.minimum_morning_level):\n",
        "                if (\n",
        "                        self.battery_level + action * self.max_power * self.charge_efficiency) < self.minimum_morning_level:  # If the chosen action will not charge the battery to 20kWh\n",
        "                    action = (self.minimum_morning_level - self.battery_level) / (\n",
        "                                self.max_power * self.charge_efficiency)  # Charge until 20kWh\n",
        "                    # charge higher to nearest legal action\n",
        "                    # action = math.ceil(action * 10.0) / 10.0\n",
        "\n",
        "            elif action < 0:\n",
        "                if (self.battery_level + action * self.max_power) < self.minimum_morning_level:\n",
        "                    if self.battery_level < self.minimum_morning_level:  # If the level was lower than 20kWh, charge until 20kWh\n",
        "                        action = (self.minimum_morning_level - self.battery_level) / (\n",
        "                                    self.max_power * self.charge_efficiency)  # Charge until 20kWh\n",
        "                        # charge higher to nearest legal action\n",
        "                        # action = math.ceil(action * 10.0) / 10.0\n",
        "                    elif self.battery_level >= self.minimum_morning_level:  # If the level was higher than 20kWh, discharge until 20kWh\n",
        "                        action = (self.minimum_morning_level - self.battery_level) / (\n",
        "                            self.max_power)  # Discharge until 20kWh\n",
        "                        # discharge less to keep higher than 20kwh\n",
        "                        # action = math.ceil(action * 10.0) / 10.0\n",
        "            elif action == 0:\n",
        "                if self.battery_level < self.minimum_morning_level:\n",
        "                    action = (self.minimum_morning_level - self.battery_level) / (\n",
        "                                self.max_power * self.charge_efficiency)\n",
        "                    # charge higher to nearest legal action\n",
        "                    # action = math.ceil(action * 10.0) / 10.0\n",
        "                    \n",
        "        if abs(action_bs - action) != 0:   \n",
        "            #* np.exp(abs(action_bs - action))\n",
        "            penalty += 1000.0 if self.penalty_on else 0.\n",
        "\n",
        "        # if it is 8am, decide whether car will go away or stay (by random chance)\n",
        "        if self.hour == 8:\n",
        "            # self.car_is_available = np.random.choice([True, False])\n",
        "            if not self.car_is_available:\n",
        "                self.battery_level -= self.car_use_consumption\n",
        "                \n",
        "        # if self.hour == 18:\n",
        "        #     self.car_is_available = True\n",
        "            \n",
        "        # convert action [-1,1] to actual kwh charging (transform to [-25,25])\n",
        "        # if action > 0:\n",
        "        #   action = action * self.max_power * self.charge_efficiency\n",
        "        # # action = action * self.max_charging_level # now action lies in [-25,25]\n",
        "        # else:\n",
        "        #   action = action * self.max_power\n",
        "\n",
        "        # no charging/discharging if car is away, besides reduction of batter level by 20kwh at 8am (enforced later on)\n",
        "        if not self.car_is_available: # check if car is currently away, enforced by overriding previous if statement\n",
        "            action = 0\n",
        "            if abs(action_bs - action) != 0:   \n",
        "                #* np.exp(abs(action_bs - action))\n",
        "                penalty += 1000.0 if self.penalty_on else 0.\n",
        "\n",
        "\n",
        "        # Calculate the costs and battery level when charging (action >0)\n",
        "        if (action > 0) and (self.battery_level <= self.battery_capacity):\n",
        "            if (self.battery_level + action * self.max_power * self.charge_efficiency) > self.battery_capacity:\n",
        "                action = (self.battery_capacity - self.battery_level) / (self.max_power * self.charge_efficiency)\n",
        "                # charge less to nearest legal action\n",
        "                # action = math.floor(action * 10.0) / 10.0\n",
        "            charged_electricity_kW = action * self.max_power\n",
        "            charged_electricity_costs = charged_electricity_kW * self.price_values[self.day - 1][\n",
        "                self.hour - 1] * 2 * 1e-3\n",
        "            if abs(action_bs - action) != 0:\n",
        "                penalty += 1000.0 if self.penalty_on else 0.\n",
        "            reward = -charged_electricity_costs - penalty\n",
        "            self.battery_level += charged_electricity_kW * self.charge_efficiency\n",
        "\n",
        "        # Calculate the profits and battery level when discharging (action <0)\n",
        "        elif (action < 0) and (self.battery_level >= 0):\n",
        "            if (self.battery_level + action * self.max_power) < 0:\n",
        "                action = -self.battery_level / (self.max_power)\n",
        "                # discharge less to nearest legal action\n",
        "                # action = math.ceil(action * 10.0) / 10.0\n",
        "            if abs(action_bs - action) != 0:\n",
        "                penalty += 1000.0 if self.penalty_on else 0\n",
        "                \n",
        "            discharged_electricity_kWh = action * self.max_power  # Negative discharge value\n",
        "            discharged_electricity_profits = abs(discharged_electricity_kWh) * self.discharge_efficiency * \\\n",
        "                                             self.price_values[self.day - 1][self.hour - 1] * 1e-3\n",
        "                                             \n",
        "            reward = discharged_electricity_profits - penalty\n",
        "            self.battery_level += discharged_electricity_kWh\n",
        "            # Some small numerical errors causing the battery level to be 1e-14 to 1e-17 under 0 :\n",
        "            if self.battery_level < 0:\n",
        "                self.battery_level = 0\n",
        "\n",
        "        else:\n",
        "            reward = 0.\n",
        "            \n",
        "\n",
        "        # # meeting constraints of having 20kwh at 7am, and alwys more than 0kwh stored\n",
        "        # if self.car_is_available:\n",
        "        #   low = self.cont_action_space.low[0]\n",
        "        #   high = self.cont_action_space.high[0]\n",
        "        #   # calculate the exact action you need to get the battery to 20kwh, by taking the min of all sufficient (=legal) actions\n",
        "        #   if self.hour == 7: # adapt legal action calculating functions\n",
        "        #     legal_actions = get_legal_range([low, high], self.battery_level, min_level = 20)\n",
        "        #     if not (legal_actions[0] <= action <= legal_actions[1]): # check if action chosen by agent ensures 20kwh of battery at 7am\n",
        "        #       action = legal_actions[0] # charge needed for 20kwh, if initial action would result in <20 charge\n",
        "        #   # rest of the day, ensure that battery stays above or at 0kwh\n",
        "        #   else:\n",
        "        #     legal_actions = get_legal_range([self.cont_action_space.low[0], self.cont_action_space.high[0]], self.battery_level, min_level=0)\n",
        "        #     if not (legal_actions[0] <= action <= legal_actions[1]): # check if action chosen by agent ensures >=0kwh of battery at 7am\n",
        "        #       action = legal_actions[0] # charge needed to have positive >=0kwh charge, if initial action would result in negative <0kwh\n",
        "\n",
        "        # removing 20kwh from battery when going away from home\n",
        "        # if not self.car_is_available:\n",
        "        #   self.battery_level = self.battery_level - 20 if self.hour == 8 else self.battery_level\n",
        "\n",
        "        # update battery level based on the picked acion at timestep t\n",
        "        # if self.car_is_available:\n",
        "        #     self.battery_level += action\n",
        "\n",
        "        # no over-charging above 50kwh\n",
        "        # if self.battery_level >= 50:\n",
        "        #     self.battery_level = 50\n",
        "\n",
        "        # should we move the constraints not lower 0 here ??\n",
        "\n",
        "        ######### update the state for next hour, timestep t+1\n",
        "        # keep the hour 8 ~ 18 same, otherwise athome = True\n",
        "        # self.car_is_available = self.car_is_available if 8 <= self.hour < 18 else 1 # TO-DO: check if this should not be <= 18 instead\n",
        "\n",
        "        # reward calculations\n",
        "        # hourly_price = self.price_values[self.day-1][self.hour-1]\n",
        "        # cost_factor = 1.0 if action < 0 else 2.0\n",
        "        # efficiency_price_factor = 0.9 if action < 0 else 1.0 # obtained electricity is impacted by 0.9 when selling\n",
        "\n",
        "        # get amount of kwh bought during this step\n",
        "        # kwhs_charged = action # just for reading clarity\n",
        "        # price_of_charging = kwhs_charged * cost_factor * efficiency_price_factor * (hourly_price/1000) # go from MWh to KWh, multiply by 2 if we are buying\n",
        "        # # reward, based on current price from table\n",
        "        # reward = (-1.) * price_of_charging # reward for this step, positive if selling, negative when buying\n",
        "\n",
        "        # update counter and time variables\n",
        "        self.counter += 1 # update continuous counter (running from 0 - len(df)*24)\n",
        "        self.hour += 1 # increment hour (running from 1-24 and back to 1 after the day passed)\n",
        "\n",
        "        if self.counter % 24 == 0: # check if day is over, meaning that it is midnight at timestep t+1\n",
        "            self.hour = 1 # reset hour of the day for next timestep\n",
        "            self.day += 1 # increment to start of next day\n",
        "             \n",
        "        if self.hour == 8:\n",
        "            self.car_is_available = np.random.choice([True, False])\n",
        "    \n",
        "        if self.hour == 18:\n",
        "            self.car_is_available = True\n",
        "\n",
        "        # update state\n",
        "        self.state = self.observation()\n",
        "\n",
        "\n",
        "        # check if all hours in the dataset have been seen\n",
        "        # self.done = self.counter == self.nr_hours - 1\n",
        "        terminated = self.counter == self.nr_hours - 1\n",
        "        truncated = False\n",
        "        \n",
        "        info = action\n",
        "\n",
        "        return self.state, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "      \n",
        "    def observation(self):  # Returns the current state\n",
        "        battery_level = self.battery_level\n",
        "        price = self.price_values[self.day - 1][self.hour - 1]\n",
        "        # print(price.max(), price.min())\n",
        "        hour = self.hour\n",
        "        day_of_week = self.timestamps[self.day - 1].dayofweek  # Monday = 0, Sunday = 6\n",
        "        day_of_year = self.timestamps[self.day - 1].dayofyear  # January 1st = 1, December 31st = 365\n",
        "        month = self.timestamps[self.day - 1].month  # January = 1, December = 12\n",
        "        year = self.timestamps[self.day - 1].year\n",
        "        self.state = np.array([\n",
        "              battery_level, \n",
        "              price, \n",
        "              int(hour), \n",
        "            #   int(self.car_is_available),\n",
        "            #   int(day_of_week), \n",
        "              #int(day_of_year), \n",
        "            #   int(month), \n",
        "              #int(year),\n",
        "             ])\n",
        "        # if match  \n",
        "        # print(self.state)\n",
        "        # print(f\"{self.timestamps[self.day - 1]} --   {day_of_week}\")\n",
        "        return self.state\n",
        "\n",
        "    def reset(self):\n",
        "      self.done = False # indicates whether trajectory (run through entire dataset) has finished, analogous to 'terminated' argument\n",
        "      self.counter = 0\n",
        "      self.hour = 1\n",
        "      self.day = 1\n",
        "      # self.car_is_available = True\n",
        "      # self.at_home = self.car_is_available  \n",
        "\n",
        "        # resetting battery charge state var, random re-initialization between 0 and 50kwh\n",
        "        #   if self.hour == 8:\n",
        "        #     self.battery_level = np.random.uniform(self.minimum_morning_level, self.battery_capacity) # continuous value between 20 and 50\n",
        "        #   elif 8 < self.hour <= 18:\n",
        "        #     self.battery_level = np.random.uniform(0, self.battery_capacity-self.minimum_morning_level) # continuous value between 20 and 50\n",
        "        #   else:\n",
        "      self.battery_level = np.random.uniform(0, self.battery_capacity) # continuous value between 0 and 50\n",
        "        \n",
        "      # randomly re-initialize athome with 50/50 chance of being away at hour 0 (8am) or not\n",
        "\n",
        "      self.car_is_available = True  # np.random.randint(self.position_space.n) # 50% chance at home at start of each day\n",
        "      # new initial state\n",
        "      self.state = self.observation()\n",
        "    #   self.state = np.array([self.battery_level, self.hour])\n",
        "\n",
        "      return self.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QAgent():\n",
        "    \n",
        "    def __init__(self, env, bin_size = {'battery': 6, 'price': 3,'hour': 3, 'action': 3},\n",
        "                  properties = {'reward_shaping':True, 'penalties':True, 'nr_simulations':10, 'discount_rate':0.95}):\n",
        "        \n",
        "        '''\n",
        "        Params:\n",
        "        \n",
        "        env_name = name of the specific environment that the agent wants to solve\n",
        "        discount_rate = discount rate used for future rewards\n",
        "        bin_size = number of bins used for discretizing the state space\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        #create an environment\n",
        "        self.env = env\n",
        "        \n",
        "        # get all agent properties\n",
        "        self.properties = properties\n",
        "        self.discount_rate = self.properties['discount_rate']\n",
        "        self.properties_path = f\"disc_{self.properties['discount_rate']}_shap_{self.properties['reward_shaping']}_pen_{self.properties['penalties']}_sims_{self.properties['nr_simulations']}\"\n",
        "\n",
        "        # get all the bin sizes\n",
        "        self.bin_size = bin_size\n",
        "        \n",
        "        #The algorithm has then 3 different actions\n",
        "        #0: Accelerate to the left\n",
        "        #1: Don't accelerate\n",
        "        #2: Accelerate to the right\n",
        "        # self.action_space = self.env.action_space.n\n",
        "        \n",
        "\n",
        "        # self.action_repr = self.env.action_space\n",
        "        # self.actions = Discrete(len(self.action_repr))\n",
        "        self.action_space = self.env.action_space_n\n",
        "        values_price = self.env.price_values.flatten()\n",
        "        self.bin_prices = {\n",
        "            '3': np.quantile(values_price, [0, 0.5, 0.99]),\n",
        "            '4': np.quantile(values_price, [0, 0.33, 0.66, 0.99]),\n",
        "            '5': np.quantile(values_price, [0, 0.25, 0.5, 0.75, 0.99]),\n",
        "            '7': np.quantile(values_price, [0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.99]),\n",
        "            '11': np.quantile(values_price, [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]),   \n",
        "        }\n",
        "\n",
        "        # self.bin_hours = {\n",
        "        #     '3': np.quantile(values_price, [0, 0.5, 1]),\n",
        "        #     '5': np.quantile(values_price, [0, 0.25, 0.5, 0.75, 1]),\n",
        "        #     '7': np.quantile(values_price, [0, 0.2, 0.4, 0.6, 0.7, 0.8, 1]),\n",
        "        #     '11': np.quantile(values_price, [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]),   \n",
        "        # }\n",
        "        \n",
        "        # print(self.action_space, self.bin_size)\n",
        "        \n",
        "        #State incorporates the observation state\n",
        "        #State[0] is x position\n",
        "        #State[1] is velocity\n",
        "    \n",
        "        #Get the low and high values of the environment space\n",
        "        # self.low = self.env.observation_space.low\n",
        "        # self.high = self.env.observation_space.high\n",
        "        \n",
        "        \n",
        "        self.battery_low = self.env.battery_range.low[0]\n",
        "        self.battery_high = self.env.battery_range.high[0]\n",
        "        self.hour_low = 0\n",
        "        self.hour_high = 24\n",
        "        \n",
        "        #Create bins for both observation features\n",
        "        self.bin_battery = np.linspace(self.battery_low, self.battery_high, self.bin_size['battery'])\n",
        "        print(f\"Battery Levels ({self.bin_size['battery']} bins): {self.bin_battery}\")\n",
        "        # 0.01\n",
        "        # confidence interval 99%\n",
        "        # print(np.quantile(values_price, 0.99))\n",
        "        # print(str(self.bin_size['price']))\n",
        "        # print(self.bin_prices[str(self.bin_size['price'])])\n",
        "        # self.bin_price = np.linspace(0, np.quantile(values_price, 0.99), self.bin_prices[str(self.bin_size['price'])])\n",
        "        # self.bin_price = np.concatenate((np.array([0]), self.bin_prices[str(self.bin_size['price'])][1:]))\n",
        "        self.bin_price = self.bin_prices[str(self.bin_size['price'])]\n",
        "        print(f\"Prices ({str(self.bin_size['price'])} bins): {self.bin_price}\")\n",
        "        \n",
        "        self.bin_hour = np.linspace(self.hour_low, self.hour_high, self.bin_size['hour'] + 1)\n",
        "        print(f\"Hours ({self.bin_size['hour']} bins): {self.bin_hour}\")\n",
        "        '''\n",
        "        ToDo:\n",
        "        \n",
        "        Please create the bins for the velocity feature in the same manner and call this variable self.bin_velocity!\n",
        "        '''\n",
        "        \n",
        "        #Solution\n",
        "        # self.bin_hour = np.linspace(self.hour_low, self.hour_high, self.bin_size['hour']) \n",
        "        \n",
        "        #Append the two bins\n",
        "        self.bins = [self.bin_battery, self.bin_price, self.bin_hour]\n",
        "        \n",
        "        self.total_output = {}\n",
        "        \n",
        "    \n",
        "    def get_shaping_reward(self, state, next_state):\n",
        "        battery_level, electricity_price, hour = state[0], state[1], state[2]\n",
        "        next_battery_level, next_electricity_price, next_hour = next_state[0], next_state[1], next_state[2]\n",
        "\n",
        "        extra_reward = 0.0\n",
        "        weight = 100.0   \n",
        "         \n",
        "        if next_battery_level < battery_level: # sell\n",
        "            extra_reward = (electricity_price - 2 * next_electricity_price) * (battery_level - next_battery_level)\n",
        "        if next_battery_level > battery_level: # buy\n",
        "            extra_reward = (2 * electricity_price - next_electricity_price) * (battery_level - next_battery_level)\n",
        "        \n",
        "        return weight * extra_reward\n",
        "    \n",
        "    def discretize_state(self, state):\n",
        "        \n",
        "        '''\n",
        "        Params:\n",
        "        state = state observation that needs to be discretized\n",
        "        \n",
        "        \n",
        "        Returns:\n",
        "        discretized state\n",
        "        '''\n",
        "        #Now we can make use of the function np.digitize and bin it\n",
        "        self.state = state\n",
        "        # print(f\"self.state: {self.state}\")\n",
        "        \n",
        "        #Create an empty state\n",
        "        digitized_state = []\n",
        "    \n",
        "        # (-inf, 0) [0, 1)    [50, )  # 52 states\n",
        "        digitized_state.append(np.digitize(self.state[0], self.bins[0], right=False) -1) \n",
        "        digitized_state.append(np.digitize(self.state[1], self.bins[1], right=False) -1)\n",
        "        digitized_state.append(np.digitize(self.state[2], self.bins[2], right=True) -1)\n",
        "        \n",
        "\n",
        "        #Returns the discretized state from an observation\n",
        "        return digitized_state\n",
        "    \n",
        "    \n",
        "    def create_Q_table(self):\n",
        "        # self.state_space = self.bin_size - 1\n",
        "        #Initialize all values in the Q-table to zero\n",
        "        self.state_battery_space = self.bin_size['battery']\n",
        "        self.state_hour_space = self.bin_size['hour']\n",
        "        self.state_price_space = self.bin_size['price']\n",
        "\n",
        "        '''\n",
        "        ToDo:\n",
        "        Initialize a zero matrix of dimension state_space * state_space * action_space and call it self.Qtable!\n",
        "        '''\n",
        "        \n",
        "        #Solution:\n",
        "        # self.Qtable = np.zeros((self.state_space, self.state_space, self.action_space))\n",
        "        self.Qtable = np.zeros((self.state_battery_space, \n",
        "                                self.state_price_space,\n",
        "                                self.state_hour_space,\n",
        "                                self.action_space))\n",
        "        # print(self.Qtable.shape)\n",
        "        \n",
        "    def save_Q_table(self):\n",
        "        table_shape = self.Qtable.shape\n",
        "        \n",
        "        num_battery_levels = table_shape[0]\n",
        "        num_price_levels = table_shape[1]\n",
        "        num_hours = table_shape[2]\n",
        "        num_actions = table_shape[3]\n",
        "        \n",
        "        filename = f\"qtable_{self.properties_path}_batt_{num_battery_levels}_price_{num_price_levels}_hour_{num_hours}_action_{num_actions}.npy\"\n",
        "        self.k = filename\n",
        "        np.save(f\"../../../data/{filename}\", self.Qtable)\n",
        "        print(f\"{filename} is saved ...\")\n",
        "        \n",
        "    def load_Q_table(self):\n",
        "        filename = f\"qtable_{self.properties_path}_batt_{self.bin_size['battery']}_price_{self.bin_size['price']}_hour_{self.bin_size['hour']}_action_{self.bin_size['action']}.npy\"\n",
        "        qtable = np.load(f\"../../../data/{filename}\")\n",
        "        return qtable\n",
        "\n",
        "    def train(self, learning_rate, epsilon = 0.05, epsilon_decay = 200, adaptive_epsilon = False, \n",
        "              adapting_learning_rate = False):\n",
        "        \n",
        "        '''\n",
        "        Params:\n",
        "        \n",
        "        simulations = number of episodes of a game to run\n",
        "        learning_rate = learning rate for the update equation\n",
        "        epsilon = epsilon value for epsilon-greedy algorithm\n",
        "        epsilon_decay = number of full episodes (games) over which the epsilon value will decay to its final value\n",
        "        adaptive_epsilon = boolean that indicates if the epsilon rate will decay over time or not\n",
        "        adapting_learning_rate = boolean that indicates if the learning rate should be adaptive or not\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        #Initialize variables that keep track of the rewards\n",
        "        self.rewards = []\n",
        "        self.average_rewards = []\n",
        "        self.num_simulations = []\n",
        "        \n",
        "        #Call the Q table function to create an initialized Q table\n",
        "        self.create_Q_table()\n",
        "        \n",
        "        #Set epsilon rate, epsilon decay and learning rate\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        #Set start epsilon, so here we want a starting exploration rate of 1\n",
        "        self.epsilon_start = 1\n",
        "        self.epsilon_end = 0.05\n",
        "        \n",
        "        #If we choose adaptive learning rate, we start with a value of 1 and decay it over time!\n",
        "        if adapting_learning_rate:\n",
        "            self.learning_rate = 1\n",
        "        \n",
        "        for i in range(self.properties['nr_simulations']):\n",
        "            # initialize output for current simulation\n",
        "            self.total_output[i] = {'rewards': [], 'actions': [], 'battery_levels': []}\n",
        "            \n",
        "            if i % 10 == 0:\n",
        "                print(f'Please wait, the algorithm is learning! The current simulation is {i}')\n",
        "            # print(f'Please wait, the algorithm is learning! The current simulation is {i}')           \n",
        "            #Initialize the state\n",
        "            # state = self.env.reset()[0]   # reset returns a dict, need to take the 0th entry.\n",
        "            state = self.env.reset()\n",
        "        \n",
        "            #Set a variable that flags if an episode has terminated\n",
        "            done = False\n",
        "        \n",
        "            #Discretize the state space\n",
        "            # continous_state = state\n",
        "            state = self.discretize_state(state)\n",
        "            \n",
        "            \n",
        "            #Set the rewards to 0\n",
        "            total_rewards = 0\n",
        "            \n",
        "            #If adaptive epsilon rate\n",
        "\n",
        "            # epsilon = 0.05\n",
        "            # epsilon_decay = 1000\n",
        "            \n",
        "            # self.epsilon_start = 1\n",
        "            # self.epsilon_end = 0.05\n",
        "            if adaptive_epsilon:\n",
        "                self.epsilon = np.interp(i, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
        "                \n",
        "                #Logging just to check it decays as we want it to do, we just print out the first three statements\n",
        "                if i % 5 == 0 and i <= 100:\n",
        "                    print(f\"The current epsilon rate is {self.epsilon}\")\n",
        "                \n",
        "            #Loop until an episode has terminated\n",
        "            while not done:\n",
        "                \n",
        "                #Pick an action based on epsilon greedy\n",
        "                \n",
        "                '''\n",
        "                ToDo: Write the if statement that picks a random action\n",
        "                Tip: Make use of np.random.uniform() and the self.epsilon to make a decision!\n",
        "                Tip: You can also make use of the method sample() of the self.env.action_space \n",
        "                    to generate a random action!\n",
        "                '''\n",
        "                \n",
        "                #Solution:\n",
        "                \n",
        "                #Pick random action\n",
        "                if np.random.uniform(0, 1) > 1-self.epsilon:\n",
        "                    #This picks a random action from 0,1,2\n",
        "                    # action = self.env.action_space.sample()\n",
        "                    action = np.random.choice(self.env.action_space)\n",
        "                    action_i = np.where(self.env.action_space == action)[0][0]   \n",
        "                #Pick a greedy action\n",
        "                else:\n",
        "                    action_i = np.argmax(self.Qtable[state[0],\n",
        "                                                     state[1],\n",
        "                                                     state[2],\n",
        "                                                     :])\n",
        "                    action = self.env.action_space[action_i]\n",
        "\n",
        "                # Correct small numerical errors :  \n",
        "                action = round(action, 3)\n",
        "                    \n",
        "                #Now sample the next_state, reward, done and info from the environment\n",
        "                \n",
        "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "                done =  terminated or truncated\n",
        "                \n",
        "\n",
        "                # continous_next_state = next_state\n",
        "                next_state = self.discretize_state(next_state)\n",
        "                # print(next_state)\n",
        "                #Target value \n",
        "\n",
        "                Q_target = (reward + self.discount_rate*np.max(self.Qtable[next_state[0], \n",
        "                                                                           next_state[1],\n",
        "                                                                           next_state[2],\n",
        "                                                                           ]))\n",
        "                \n",
        "                # enable/disable reward shaping\n",
        "                if self.properties[\"reward_shaping\"]:\n",
        "                    Q_target += self.get_shaping_reward(state, next_state)\n",
        "                \n",
        "                #Calculate the Temporal difference error (delta)\n",
        "                delta = self.learning_rate * (Q_target - self.Qtable[state[0], \n",
        "                                                                     state[1], \n",
        "                                                                     state[2],     \n",
        "                                                                     action_i])\n",
        "                \n",
        "                #Update the Q-value\n",
        "                self.Qtable[state[0], \n",
        "                            state[1], \n",
        "                            state[2], \n",
        "                            action_i] = self.Qtable[state[0], \n",
        "                                                    state[1], \n",
        "                                                    state[2], \n",
        "                                                    action_i] + delta\n",
        "                \n",
        "                # update the reward and the hyperparameters\n",
        "                # and transform from numpy dtypes to native python types if necessary (to allow JSON encoding) \n",
        "\n",
        "                # print('reward', reward, type(reward))\n",
        "                # print('action', action, type(action))\n",
        "                # print('battery', state[0], type(state[0]))\n",
        "                self.total_output[i]['rewards'] += [float(reward)] if not isinstance(reward,float) else [reward]\n",
        "                self.total_output[i]['actions'] += [float(action)] if not isinstance(action,float) else [action]\n",
        "                self.total_output[i]['battery_levels'] += [int(state[0])] if not isinstance(state[0],int) else [state[0]]\n",
        "                \n",
        "                total_rewards += reward\n",
        "\n",
        "                # update state to next state\n",
        "                state = next_state\n",
        "                \n",
        "                \n",
        "            \n",
        "            if adapting_learning_rate:\n",
        "                self.learning_rate = self.learning_rate/np.sqrt(i+1)\n",
        "            \n",
        "            self.rewards.append(total_rewards)\n",
        "            \n",
        "            #Calculate the average score over 100 episodes\n",
        "            if i % 10 == 0:\n",
        "                self.average_rewards.append(np.mean(self.rewards))\n",
        "                self.num_simulations.append(i+1)\n",
        "                \n",
        "                #Initialize a new reward list, as otherwise the average values would reflect all rewards!\n",
        "                self.rewards = []\n",
        "        \n",
        "        print('The simulation is done!')\n",
        "        \n",
        "\n",
        "        \n",
        "    def visualize_rewards(self):\n",
        "        plt.figure(figsize =(7.5,7.5))\n",
        "        plt.plot(self.num_simulations, self.average_rewards)\n",
        "        # plt.axhline(y = -110, color = 'r', linestyle = '-')\n",
        "        plt.title('Average reward over the past 10 simulations', fontsize = 10)\n",
        "        # plt.legend(['Q-learning performance','Benchmark'])\n",
        "        plt.xlabel('Number of simulations', fontsize = 10)\n",
        "        plt.ylabel('Average reward', fontsize = 10)\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space (3 actions): [-1  0  1]\n",
            "Battery Levels (6 bins): [ 0. 10. 20. 30. 40. 50.]\n",
            "Prices (3 bins): [1.0e-02 4.3e+01 1.5e+02]\n",
            "Hours (3 bins): [ 0.  8. 16. 24.]\n",
            "Please wait, the algorithm is learning! The current simulation is 0\n",
            "The current epsilon rate is 1.0\n",
            "Please wait, the algorithm is learning! The current simulation is 1\n",
            "The current epsilon rate is 0.99525\n",
            "Please wait, the algorithm is learning! The current simulation is 2\n",
            "The current epsilon rate is 0.9905\n",
            "Please wait, the algorithm is learning! The current simulation is 3\n",
            "The current epsilon rate is 0.98575\n",
            "Please wait, the algorithm is learning! The current simulation is 4\n",
            "The current epsilon rate is 0.981\n",
            "Please wait, the algorithm is learning! The current simulation is 5\n",
            "The current epsilon rate is 0.97625\n",
            "Please wait, the algorithm is learning! The current simulation is 6\n",
            "The current epsilon rate is 0.9715\n",
            "Please wait, the algorithm is learning! The current simulation is 7\n",
            "The current epsilon rate is 0.96675\n",
            "Please wait, the algorithm is learning! The current simulation is 8\n",
            "The current epsilon rate is 0.962\n",
            "Please wait, the algorithm is learning! The current simulation is 9\n",
            "The current epsilon rate is 0.95725\n",
            "Please wait, the algorithm is learning! The current simulation is 10\n",
            "The current epsilon rate is 0.9525\n",
            "Please wait, the algorithm is learning! The current simulation is 11\n",
            "The current epsilon rate is 0.94775\n",
            "Please wait, the algorithm is learning! The current simulation is 12\n",
            "The current epsilon rate is 0.9430000000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 13\n",
            "The current epsilon rate is 0.93825\n",
            "Please wait, the algorithm is learning! The current simulation is 14\n",
            "The current epsilon rate is 0.9335\n",
            "Please wait, the algorithm is learning! The current simulation is 15\n",
            "The current epsilon rate is 0.92875\n",
            "Please wait, the algorithm is learning! The current simulation is 16\n",
            "The current epsilon rate is 0.924\n",
            "Please wait, the algorithm is learning! The current simulation is 17\n",
            "The current epsilon rate is 0.91925\n",
            "Please wait, the algorithm is learning! The current simulation is 18\n",
            "The current epsilon rate is 0.9145\n",
            "Please wait, the algorithm is learning! The current simulation is 19\n",
            "The current epsilon rate is 0.9097500000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 20\n",
            "The current epsilon rate is 0.905\n",
            "Please wait, the algorithm is learning! The current simulation is 21\n",
            "The current epsilon rate is 0.90025\n",
            "Please wait, the algorithm is learning! The current simulation is 22\n",
            "The current epsilon rate is 0.8955\n",
            "Please wait, the algorithm is learning! The current simulation is 23\n",
            "The current epsilon rate is 0.89075\n",
            "Please wait, the algorithm is learning! The current simulation is 24\n",
            "The current epsilon rate is 0.886\n",
            "Please wait, the algorithm is learning! The current simulation is 25\n",
            "The current epsilon rate is 0.88125\n",
            "Please wait, the algorithm is learning! The current simulation is 26\n",
            "The current epsilon rate is 0.8765000000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 27\n",
            "The current epsilon rate is 0.87175\n",
            "Please wait, the algorithm is learning! The current simulation is 28\n",
            "The current epsilon rate is 0.867\n",
            "Please wait, the algorithm is learning! The current simulation is 29\n",
            "The current epsilon rate is 0.86225\n",
            "Please wait, the algorithm is learning! The current simulation is 30\n",
            "The current epsilon rate is 0.8575\n",
            "Please wait, the algorithm is learning! The current simulation is 31\n",
            "The current epsilon rate is 0.85275\n",
            "Please wait, the algorithm is learning! The current simulation is 32\n",
            "The current epsilon rate is 0.848\n",
            "Please wait, the algorithm is learning! The current simulation is 33\n",
            "The current epsilon rate is 0.84325\n",
            "Please wait, the algorithm is learning! The current simulation is 34\n",
            "The current epsilon rate is 0.8385\n",
            "Please wait, the algorithm is learning! The current simulation is 35\n",
            "The current epsilon rate is 0.83375\n",
            "Please wait, the algorithm is learning! The current simulation is 36\n",
            "The current epsilon rate is 0.829\n",
            "Please wait, the algorithm is learning! The current simulation is 37\n",
            "The current epsilon rate is 0.82425\n",
            "Please wait, the algorithm is learning! The current simulation is 38\n",
            "The current epsilon rate is 0.8195\n",
            "Please wait, the algorithm is learning! The current simulation is 39\n",
            "The current epsilon rate is 0.81475\n",
            "Please wait, the algorithm is learning! The current simulation is 40\n",
            "The current epsilon rate is 0.81\n",
            "Please wait, the algorithm is learning! The current simulation is 41\n",
            "The current epsilon rate is 0.80525\n",
            "Please wait, the algorithm is learning! The current simulation is 42\n",
            "The current epsilon rate is 0.8005\n",
            "Please wait, the algorithm is learning! The current simulation is 43\n",
            "The current epsilon rate is 0.79575\n",
            "Please wait, the algorithm is learning! The current simulation is 44\n",
            "The current epsilon rate is 0.791\n",
            "Please wait, the algorithm is learning! The current simulation is 45\n",
            "The current epsilon rate is 0.78625\n",
            "Please wait, the algorithm is learning! The current simulation is 46\n",
            "The current epsilon rate is 0.7815\n",
            "Please wait, the algorithm is learning! The current simulation is 47\n",
            "The current epsilon rate is 0.77675\n",
            "Please wait, the algorithm is learning! The current simulation is 48\n",
            "The current epsilon rate is 0.772\n",
            "Please wait, the algorithm is learning! The current simulation is 49\n",
            "The current epsilon rate is 0.76725\n",
            "Please wait, the algorithm is learning! The current simulation is 50\n",
            "The current epsilon rate is 0.7625\n",
            "Please wait, the algorithm is learning! The current simulation is 51\n",
            "The current epsilon rate is 0.75775\n",
            "Please wait, the algorithm is learning! The current simulation is 52\n",
            "The current epsilon rate is 0.753\n",
            "Please wait, the algorithm is learning! The current simulation is 53\n",
            "The current epsilon rate is 0.7482500000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 54\n",
            "The current epsilon rate is 0.7435\n",
            "Please wait, the algorithm is learning! The current simulation is 55\n",
            "The current epsilon rate is 0.73875\n",
            "Please wait, the algorithm is learning! The current simulation is 56\n",
            "The current epsilon rate is 0.734\n",
            "Please wait, the algorithm is learning! The current simulation is 57\n",
            "The current epsilon rate is 0.72925\n",
            "Please wait, the algorithm is learning! The current simulation is 58\n",
            "The current epsilon rate is 0.7245\n",
            "Please wait, the algorithm is learning! The current simulation is 59\n",
            "The current epsilon rate is 0.71975\n",
            "Please wait, the algorithm is learning! The current simulation is 60\n",
            "The current epsilon rate is 0.7150000000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 61\n",
            "The current epsilon rate is 0.71025\n",
            "Please wait, the algorithm is learning! The current simulation is 62\n",
            "The current epsilon rate is 0.7055\n",
            "Please wait, the algorithm is learning! The current simulation is 63\n",
            "The current epsilon rate is 0.70075\n",
            "Please wait, the algorithm is learning! The current simulation is 64\n",
            "The current epsilon rate is 0.696\n",
            "Please wait, the algorithm is learning! The current simulation is 65\n",
            "The current epsilon rate is 0.69125\n",
            "Please wait, the algorithm is learning! The current simulation is 66\n",
            "The current epsilon rate is 0.6865\n",
            "Please wait, the algorithm is learning! The current simulation is 67\n",
            "The current epsilon rate is 0.6817500000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 68\n",
            "The current epsilon rate is 0.677\n",
            "Please wait, the algorithm is learning! The current simulation is 69\n",
            "The current epsilon rate is 0.67225\n",
            "Please wait, the algorithm is learning! The current simulation is 70\n",
            "The current epsilon rate is 0.6675\n",
            "Please wait, the algorithm is learning! The current simulation is 71\n",
            "The current epsilon rate is 0.66275\n",
            "Please wait, the algorithm is learning! The current simulation is 72\n",
            "The current epsilon rate is 0.658\n",
            "Please wait, the algorithm is learning! The current simulation is 73\n",
            "The current epsilon rate is 0.65325\n",
            "Please wait, the algorithm is learning! The current simulation is 74\n",
            "The current epsilon rate is 0.6485000000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 75\n",
            "The current epsilon rate is 0.64375\n",
            "Please wait, the algorithm is learning! The current simulation is 76\n",
            "The current epsilon rate is 0.639\n",
            "Please wait, the algorithm is learning! The current simulation is 77\n",
            "The current epsilon rate is 0.63425\n",
            "Please wait, the algorithm is learning! The current simulation is 78\n",
            "The current epsilon rate is 0.6295\n",
            "Please wait, the algorithm is learning! The current simulation is 79\n",
            "The current epsilon rate is 0.62475\n",
            "Please wait, the algorithm is learning! The current simulation is 80\n",
            "The current epsilon rate is 0.62\n",
            "Please wait, the algorithm is learning! The current simulation is 81\n",
            "The current epsilon rate is 0.6152500000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 82\n",
            "The current epsilon rate is 0.6105\n",
            "Please wait, the algorithm is learning! The current simulation is 83\n",
            "The current epsilon rate is 0.60575\n",
            "Please wait, the algorithm is learning! The current simulation is 84\n",
            "The current epsilon rate is 0.601\n",
            "Please wait, the algorithm is learning! The current simulation is 85\n",
            "The current epsilon rate is 0.59625\n",
            "Please wait, the algorithm is learning! The current simulation is 86\n",
            "The current epsilon rate is 0.5915\n",
            "Please wait, the algorithm is learning! The current simulation is 87\n",
            "The current epsilon rate is 0.58675\n",
            "Please wait, the algorithm is learning! The current simulation is 88\n",
            "The current epsilon rate is 0.5820000000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 89\n",
            "The current epsilon rate is 0.57725\n",
            "Please wait, the algorithm is learning! The current simulation is 90\n",
            "The current epsilon rate is 0.5725\n",
            "Please wait, the algorithm is learning! The current simulation is 91\n",
            "The current epsilon rate is 0.56775\n",
            "Please wait, the algorithm is learning! The current simulation is 92\n",
            "The current epsilon rate is 0.563\n",
            "Please wait, the algorithm is learning! The current simulation is 93\n",
            "The current epsilon rate is 0.55825\n",
            "Please wait, the algorithm is learning! The current simulation is 94\n",
            "The current epsilon rate is 0.5535\n",
            "Please wait, the algorithm is learning! The current simulation is 95\n",
            "The current epsilon rate is 0.5487500000000001\n",
            "Please wait, the algorithm is learning! The current simulation is 96\n",
            "The current epsilon rate is 0.544\n",
            "Please wait, the algorithm is learning! The current simulation is 97\n",
            "The current epsilon rate is 0.53925\n",
            "Please wait, the algorithm is learning! The current simulation is 98\n",
            "The current epsilon rate is 0.5345\n",
            "Please wait, the algorithm is learning! The current simulation is 99\n",
            "The current epsilon rate is 0.5297499999999999\n",
            "The simulation is done!\n",
            "qtable_disc_0.95_shap_0_pen_1_sims_100_batt_6_price_3_hour_3_action_3.npy is saved ...\n"
          ]
        }
      ],
      "source": [
        "#We can also train the Qagent with a decaying epsilon schedule\n",
        "config_file = open('../../../config.json')\n",
        "config = json.load(config_file)\n",
        "\n",
        "bin_size, properties, learning_rate = config['bin_size'], config['properties'], config['learning_rate']\n",
        "\n",
        "# num_battery_levels can be 6 11 26 51         => not test for now : 26, 51\n",
        "# num_actions can be 3, 5,                     => not test for now : 11, 21\n",
        "# num_hours can be 3, 4, 6, 12, 24             => not test for now : 12, 24\n",
        "# num price can be 3, 4, 5, 7, 11              => not test for now : 11\n",
        "\n",
        "env = StorageEnv(path_to_train_data=\"../../../data/train.xlsx\", num_actions=bin_size['action'], penalty_on=properties['penalties'])\n",
        "\n",
        "agent_epsilon_decay = QAgent(env, bin_size=bin_size, properties=properties)\n",
        "agent_epsilon_decay.train(learning_rate=learning_rate, adaptive_epsilon=True)\n",
        "agent_epsilon_decay.save_Q_table()\n",
        "\n",
        "# create file to store all rewards in if it doesnt exist yet\n",
        "if not os.path.isfile('train_rewards.txt'):\n",
        "    rewards = {}\n",
        "\n",
        "else:\n",
        "    # reading the rewards data from the file \n",
        "    with open('train_rewards.txt', 'r') as f:\n",
        "        data = f.read()\n",
        "        \n",
        "    # reconstructing the data as a dictionary \n",
        "    rewards = json.loads(data)\n",
        "\n",
        "# agent_epsilon_decay.num_simulations\n",
        "# add current run to rewards dictionary\n",
        "rewards[agent_epsilon_decay.k] = [agent_epsilon_decay.num_simulations, agent_epsilon_decay.average_rewards]# agent_epsilon_decay.total_output\n",
        "\n",
        "# save new rewards dictionary to file\n",
        "with open('train_rewards.txt', 'w') as f: \n",
        "     f.write(json.dumps(rewards))\n",
        "\n",
        "\n",
        "# with open(filename, 'w') as f:\n",
        "#     json.dump(agent_epsilon_decay.total_rewards, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get notebook version from Wenhua\n",
        "# add reward saving properly with dict, make sure rewards from train and rewards/actions/battery levels from test are being saved\n",
        "# push main.py to github\n",
        "# run best config for all experiments\n",
        "# finish plottig functios\n",
        "# write report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 750x750 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "agent_epsilon_decay.visualize_rewards()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
